import gym
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from stable_baselines3 import PPO, SAC
import sys
import os
from itertools import product
import json

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ sys.path
current_dir = os.getcwd()
sys.path.append(current_dir)

from drilling_env.drilling_env import DrillingEnv

class HyperparameterTuner:
    def __init__(self, env):
        self.env = env
        self.results = []
        
    def evaluate_hyperparameters(self, model_class, hyperparams, agent_name, num_episodes=3):
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±"""
        print(f"\n--- ØªØ³Øª {agent_name} Ø¨Ø§ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ---")
        
        # Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¨Ø§ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        model = model_class("MlpPolicy", self.env, verbose=0, **hyperparams)
        
        # Ø¢Ù…ÙˆØ²Ø´ Ú©ÙˆØªØ§Ù‡ Ù…Ø¯Øª
        model.learn(total_timesteps=50_000)
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        episode_rewards = []
        for episode in range(num_episodes):
            obs = self.env.reset()
            total_reward = 0
            steps = 0
            
            while True:
                action, _states = model.predict(obs, deterministic=True)
                obs, reward, done, info = self.env.step(action)
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            episode_rewards.append(total_reward)
        
        mean_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        
        result = {
            'agent': agent_name,
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'hyperparams': hyperparams.copy()
        }
        
        self.results.append(result)
        
        print(f"Ù¾Ø§Ø¯Ø§Ø´: {mean_reward:.2f} Â± {std_reward:.2f}")
        return result
    
    def tune_ppo(self):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ PPO"""
        print("\n=== Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ PPO ===")
        
        # ØªØ¹Ø±ÛŒÙ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        learning_rates = [0.0001, 0.0003, 0.001]
        gammas = [0.9, 0.95, 0.99]
        clip_ranges = [0.1, 0.2, 0.3]
        
        for lr, gamma, clip_range in product(learning_rates, gammas, clip_ranges):
            hyperparams = {
                'learning_rate': lr,
                'gamma': gamma,
                'clip_range': clip_range,
                'ent_coef': 0.01
            }
            
            self.evaluate_hyperparameters(PPO, hyperparams, "PPO")
    
    def tune_sac(self):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ SAC"""
        print("\n=== Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ SAC ===")
        
        # ØªØ¹Ø±ÛŒÙ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        learning_rates = [0.0001, 0.0003, 0.001]
        gammas = [0.9, 0.95, 0.99]
        tau_values = [0.005, 0.01, 0.02]
        
        for lr, gamma, tau in product(learning_rates, gammas, tau_values):
            hyperparams = {
                'learning_rate': lr,
                'gamma': gamma,
                'tau': tau,
                'ent_coef': 'auto'
            }
            
            self.evaluate_hyperparameters(SAC, hyperparams, "SAC")
    
    def find_best_hyperparameters(self):
        """ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§"""
        print("\n" + "="*60)
        print("ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§")
        print("="*60)
        
        # Ø¨Ù‡ØªØ±ÛŒÙ† PPO
        ppo_results = [r for r in self.results if r['agent'] == 'PPO']
        if ppo_results:
            best_ppo = max(ppo_results, key=lambda x: x['mean_reward'])
            print(f"\nğŸ¥‡ Ø¨Ù‡ØªØ±ÛŒÙ† PPO:")
            print(f"   Ù¾Ø§Ø¯Ø§Ø´: {best_ppo['mean_reward']:.2f} Â± {best_ppo['std_reward']:.2f}")
            print(f"   Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {best_ppo['hyperparams']}")
        
        # Ø¨Ù‡ØªØ±ÛŒÙ† SAC
        sac_results = [r for r in self.results if r['agent'] == 'SAC']
        if sac_results:
            best_sac = max(sac_results, key=lambda x: x['mean_reward'])
            print(f"\nğŸ¥ˆ Ø¨Ù‡ØªØ±ÛŒÙ† SAC:")
            print(f"   Ù¾Ø§Ø¯Ø§Ø´: {best_sac['mean_reward']:.2f} Â± {best_sac['std_reward']:.2f}")
            print(f"   Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {best_sac['hyperparams']}")
        
        # Ø¨Ù‡ØªØ±ÛŒÙ† Ú©Ù„ÛŒ
        best_overall = max(self.results, key=lambda x: x['mean_reward'])
        print(f"\nğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ú©Ù„ÛŒ: {best_overall['agent']}")
        print(f"   Ù¾Ø§Ø¯Ø§Ø´: {best_overall['mean_reward']:.2f} Â± {best_overall['std_reward']:.2f}")
        
        return best_ppo, best_sac, best_overall
    
    def plot_results(self):
        """Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù†ØªØ§ÛŒØ¬"""
        if not self.results:
            print("Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø³Ù… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return
        
        # Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ PPO Ùˆ SAC
        ppo_results = [r for r in self.results if r['agent'] == 'PPO']
        sac_results = [r for r in self.results if r['agent'] == 'SAC']
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± PPO
        if ppo_results:
            ppo_rewards = [r['mean_reward'] for r in ppo_results]
            ppo_stds = [r['std_reward'] for r in ppo_results]
            ppo_labels = [f"LR={r['hyperparams']['learning_rate']}, Î³={r['hyperparams']['gamma']}, CR={r['hyperparams']['clip_range']}" 
                         for r in ppo_results]
            
            axes[0].bar(range(len(ppo_rewards)), ppo_rewards, yerr=ppo_stds, capsize=5, alpha=0.7)
            axes[0].set_title('Ù†ØªØ§ÛŒØ¬ PPO')
            axes[0].set_ylabel('Ù¾Ø§Ø¯Ø§Ø´')
            axes[0].set_xticks(range(len(ppo_rewards)))
            axes[0].set_xticklabels(ppo_labels, rotation=45, ha='right')
            axes[0].grid(True, alpha=0.3)
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± SAC
        if sac_results:
            sac_rewards = [r['mean_reward'] for r in sac_results]
            sac_stds = [r['std_reward'] for r in sac_results]
            sac_labels = [f"LR={r['hyperparams']['learning_rate']}, Î³={r['hyperparams']['gamma']}, Ï„={r['hyperparams']['tau']}" 
                         for r in sac_results]
            
            axes[1].bar(range(len(sac_rewards)), sac_rewards, yerr=sac_stds, capsize=5, alpha=0.7, color='orange')
            axes[1].set_title('Ù†ØªØ§ÛŒØ¬ SAC')
            axes[1].set_ylabel('Ù¾Ø§Ø¯Ø§Ø´')
            axes[1].set_xticks(range(len(sac_rewards)))
            axes[1].set_xticklabels(sac_labels, rotation=45, ha='right')
            axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('hyperparameter_tuning_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("\nğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ 'hyperparameter_tuning_results.png' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    
    def save_results(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬"""
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame
        df_data = []
        for result in self.results:
            row = {
                'agent': result['agent'],
                'mean_reward': result['mean_reward'],
                'std_reward': result['std_reward']
            }
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
            for key, value in result['hyperparams'].items():
                row[f'hyperparam_{key}'] = value
            
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        df.to_csv('hyperparameter_tuning_results.csv', index=False, encoding='utf-8-sig')
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        best_ppo, best_sac, best_overall = self.find_best_hyperparameters()
        
        best_configs = {
            'best_ppo': best_ppo['hyperparams'] if best_ppo else None,
            'best_sac': best_sac['hyperparams'] if best_sac else None,
            'best_overall': {
                'agent': best_overall['agent'],
                'hyperparams': best_overall['hyperparams'],
                'mean_reward': best_overall['mean_reward']
            }
        }
        
        with open('best_hyperparameters.json', 'w', encoding='utf-8') as f:
            json.dump(best_configs, f, indent=2, ensure_ascii=False)
        
        print("\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
        print("   - hyperparameter_tuning_results.csv")
        print("   - best_hyperparameters.json")

def main():
    print("=== Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¹ÙˆØ§Ù…Ù„ ===")
    
    # Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ·
    print("Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ· DrillingEnv...")
    env = DrillingEnv()
    
    # Ø³Ø§Ø®Øª tuner
    tuner = HyperparameterTuner(env)
    
    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ PPO
    tuner.tune_ppo()
    
    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ SAC
    tuner.tune_sac()
    
    # ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§
    tuner.find_best_hyperparameters()
    
    # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±
    tuner.plot_results()
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    tuner.save_results()
    
    print("\nâœ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯!")

if __name__ == "__main__":
    main() 