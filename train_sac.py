import gym
from stable_baselines3 import SAC
import sys
import os
import json
from datetime import datetime
from pathlib import Path

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ sys.path
current_dir = os.getcwd()
sys.path.append(current_dir)

try:
    import mlflow
    from src.backend.services.mlflow_service import mlflow_service
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    print("âš ï¸ MLflow not available. Training will continue without logging.")

from drilling_env.drilling_env import DrillingEnv


def evaluate_model(model, env, num_episodes=10):
    """Evaluate model and return metrics"""
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        obs = env.reset()
        done = False
        total_reward = 0
        steps = 0
        
        while not done:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if steps > 10000:  # Safety limit
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        if (episode + 1) % 5 == 0:
            print(f"Ø§Ù¾ÛŒØ²ÙˆØ¯ {episode + 1}/{num_episodes}: Ù¾Ø§Ø¯Ø§Ø´ = {total_reward:.2f}")
    
    return {
        "mean_reward": sum(episode_rewards) / len(episode_rewards),
        "std_reward": (sum([(r - sum(episode_rewards)/len(episode_rewards))**2 for r in episode_rewards]) / len(episode_rewards))**0.5,
        "min_reward": min(episode_rewards),
        "max_reward": max(episode_rewards),
        "mean_episode_length": sum(episode_lengths) / len(episode_lengths),
    }


def main():
    print("=== Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ SAC Ø±ÙˆÛŒ Ù…Ø­ÛŒØ· Ø­ÙØ§Ø±ÛŒ ===")
    
    # Setup MLflow
    experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "i-drill-training")
    if MLFLOW_AVAILABLE:
        try:
            mlflow.set_experiment(experiment_name)
            print(f"âœ… MLflow experiment: {experiment_name}")
        except Exception as e:
            print(f"âš ï¸ MLflow setup failed: {e}")
    
    # Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ·
    print("Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ· DrillingEnv...")
    env = DrillingEnv()
    
    # Training parameters
    total_timesteps = int(os.getenv("TOTAL_TIMESTEPS", 100_000))
    learning_rate = float(os.getenv("LEARNING_RATE", 0.0003))
    gamma = float(os.getenv("GAMMA", 0.99))
    tau = float(os.getenv("TAU", 0.005))
    
    # Start MLflow run
    run_name = f"sac-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    
    if MLFLOW_AVAILABLE:
        mlflow.start_run(run_name=run_name)
        mlflow.log_params({
            "algorithm": "SAC",
            "total_timesteps": total_timesteps,
            "learning_rate": learning_rate,
            "gamma": gamma,
            "tau": tau,
            "policy": "MlpPolicy",
        })
        mlflow.set_tags({
            "model_type": "sac",
            "env": "DrillingEnv",
            "training_type": "automated",
        })
    
    try:
        # Ø³Ø§Ø®Øª Ù…Ø¯Ù„ SAC
        print("Ø³Ø§Ø®Øª Ù…Ø¯Ù„ SAC...")
        model = SAC(
            "MlpPolicy",
            env,
            learning_rate=learning_rate,
            gamma=gamma,
            tau=tau,
            verbose=1
        )
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
        print(f"Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ({total_timesteps} timesteps)...")
        model.learn(total_timesteps=total_timesteps)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
        model_dir = Path("models/sac")
        model_dir.mkdir(parents=True, exist_ok=True)
        model_path = model_dir / "sac_drilling_env"
        print(f"Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¯Ø± {model_path}...")
        model.save(str(model_path))
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
        print("Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡...")
        eval_metrics = evaluate_model(model, env, num_episodes=10)
        
        print(f"\nğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
        print(f"  Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù¾Ø§Ø¯Ø§Ø´: {eval_metrics['mean_reward']:.2f} Â± {eval_metrics['std_reward']:.2f}")
        print(f"  Ù¾Ø§Ø¯Ø§Ø´ Ø­Ø¯Ø§Ù‚Ù„: {eval_metrics['min_reward']:.2f}")
        print(f"  Ù¾Ø§Ø¯Ø§Ø´ Ø­Ø¯Ø§Ú©Ø«Ø±: {eval_metrics['max_reward']:.2f}")
        print(f"  Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ Ø§Ù¾ÛŒØ²ÙˆØ¯: {eval_metrics['mean_episode_length']:.2f}")
        
        # Log metrics to MLflow
        if MLFLOW_AVAILABLE:
            mlflow.log_metrics(eval_metrics)
            
            # Log model
            try:
                mlflow.log_artifact(str(model_path) + ".zip", artifact_path="model")
                
                # Register model
                mlflow.register_model(
                    f"runs:/{mlflow.active_run().info.run_id}/model",
                    "sac_drilling_env"
                )
                print("âœ… Ù…Ø¯Ù„ Ø¯Ø± MLflow Ø«Ø¨Øª Ø´Ø¯")
            except Exception as e:
                print(f"âš ï¸ Failed to register model in MLflow: {e}")
        
        # Save metrics to file
        metrics_file = model_dir / "training_metrics.json"
        with open(metrics_file, "w") as f:
            json.dump(eval_metrics, f, indent=2)
        
        print("\n=== Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯ ===")
        
    finally:
        if MLFLOW_AVAILABLE:
            mlflow.end_run()


if __name__ == "__main__":
    main() 