import pandas as pd
import numpy as np
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.impute import KNNImputer
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional, Any, Union
from enum import Enum
import warnings
import json
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64
import unittest

# ================================
# Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ùˆ EnumÙ‡Ø§
# ================================

class ValidationLevel(Enum):
    INFO = "INFO"
    WARNING = "WARNING" 
    CRITICAL = "CRITICAL"

class ReportLevel(Enum):
    SUMMARY = "SUMMARY"
    DETAILED = "DETAILED"
    TECHNICAL = "TECHNICAL"

@dataclass
class ValidationResult:
    """Ù†ØªÛŒØ¬Ù‡ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙÛŒÙ„Ø¯"""
    field_name: str
    is_valid: bool
    validation_type: str
    message: str
    level: ValidationLevel
    original_value: Any = None
    corrected_value: Any = None
    confidence: float = 1.0
    
    def to_dict(self):
        return {
            'field_name': self.field_name,
            'is_valid': self.is_valid,
            'validation_type': self.validation_type,
            'message': self.message,
            'level': self.level.value,
            'original_value': str(self.original_value) if self.original_value is not None else None,
            'corrected_value': str(self.corrected_value) if self.corrected_value is not None else None,
            'confidence': self.confidence
        }

@dataclass
class DataQualityReport:
    """Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡"""
    
    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ
    report_id: str
    generation_time: datetime
    data_source: str
    total_records: int
    total_fields: int
    time_range: Tuple[datetime, datetime]
    
    # Ø®Ù„Ø§ØµÙ‡ Ú©ÛŒÙÛŒØª
    overall_quality_score: float
    quality_status: str
    validation_duration: float
    
    # Ø¢Ù…Ø§Ø± Ø®Ø·Ø§Ù‡Ø§
    total_issues: int
    critical_issues: int
    warning_issues: int
    info_issues: int
    
    # ØªÙˆØ²ÛŒØ¹ Ù…Ø´Ú©Ù„Ø§Øª
    issues_by_type: Dict[str, int]
    issues_by_field: Dict[str, int]
    issues_by_severity: Dict[str, int]
    
    # Ø§ØµÙ„Ø§Ø­Ø§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡
    total_corrections: int
    correction_success_rate: float
    corrected_fields: List[str]
    
    # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
    recommendations: List[str]
    priority_actions: List[str]
    
    # Ø¬Ø²Ø¦ÛŒØ§Øª ÙÙ†ÛŒ
    validation_results: List[ValidationResult]
    correction_log: List[Dict]
    quality_scores: Dict[str, Any]
    
    # Ù…ØªØ§Ø¯ÛŒØªØ§
    report_level: ReportLevel
    validator_version: str

# ================================
# Ø´Ù…Ø§Ø±Ù‡ Û¸: Ø³ÛŒØ³ØªÙ… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ú©ÛŒÙÛŒØª
# ================================

class DataQualityScorer:
    """Ø³ÛŒØ³ØªÙ… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ú©Ù„ÛŒ Ø¯Ø§Ø¯Ù‡"""
    
    def __init__(self):
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ø®Ø·Ø§ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª)
        self.error_weights = {
            ValidationLevel.CRITICAL: 5.0,   # Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ
            ValidationLevel.WARNING: 2.0,    # Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù‡Ø´Ø¯Ø§Ø±
            ValidationLevel.INFO: 0.5        # Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ
        }
        
        # Ø§Ù‡Ù…ÛŒØª ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø³Ø§Ø³ÛŒØª Ø¯Ø± Ø¹Ù…Ù„ÛŒØ§Øª Ø­ÙØ§Ø±ÛŒ)
        self.field_importance = {
            # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ (Ø§Ù…Ù†ÛŒØª Ùˆ Ú©Ù†ØªØ±Ù„)
            'WOB': 1.0, 'RPM': 1.0, 'Standpipe_Pressure': 1.0, 
            'Torque': 1.0, 'Depth': 1.0, 'Mud_Flow_Rate': 1.0,
            
            # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… (Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯)
            'ROP': 0.9, 'Mud_Temperature': 0.9, 'Hook_Load': 0.9,
            'Vibration': 0.9, 'Gamma_Ray': 0.9, 'Resistivity': 0.9,
            
            # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ
            'Density': 0.8, 'Porosity': 0.8, 'Formation_Type': 0.8,
            'Lithology': 0.8, 'Bit_Wear': 0.8, 'Pump_Efficiency': 0.8,
            
            # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª
            'Active_Events': 0.7, 'Failed_Equipment': 0.7, 
            'Maintenance_Flag': 0.7, 'Abnormal_Condition': 0.7,
            'Layer_Name': 0.6
        }
        
        # Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ
        self.quality_thresholds = {
            'EXCELLENT': 0.9,    # 90% - Ú©ÛŒÙÛŒØª Ø¹Ø§Ù„ÛŒ
            'GOOD': 0.8,         # 80% - Ú©ÛŒÙÛŒØª Ø®ÙˆØ¨
            'ACCEPTABLE': 0.7,   # 70% - Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„
            'POOR': 0.6,         # 60% - Ú©ÛŒÙÛŒØª Ø¶Ø¹ÛŒÙ
            'CRITICAL': 0.5      # Ø²ÛŒØ± 50% - Ø¨Ø­Ø±Ø§Ù†ÛŒ
        }
    
    def calculate_record_score(self, validation_results: List[ValidationResult], 
                             total_fields: int) -> Dict[str, Any]:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯
        """
        if total_fields == 0:
            return self._get_empty_score()
        
        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„Ø¯
        field_results = {}
        for result in validation_results:
            if result.field_name not in field_results:
                field_results[result.field_name] = []
            field_results[result.field_name].append(result)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙÛŒÙ„Ø¯
        field_scores = {}
        field_issues = {}
        
        for field_name, results in field_results.items():
            field_score, field_issue = self._calculate_field_score(field_name, results)
            field_scores[field_name] = field_score
            if field_issue:
                field_issues[field_name] = field_issue
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ Ø±Ú©ÙˆØ±Ø¯
        overall_score = self._calculate_overall_score(field_scores, total_fields)
        
        # ØªØ¹ÛŒÛŒÙ† ÙˆØ¶Ø¹ÛŒØª Ú©ÛŒÙÛŒØª
        quality_status = self._determine_quality_status(overall_score)
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
        problematic_fields = self._identify_problematic_fields(field_scores, field_issues)
        
        return {
            'overall_score': overall_score,
            'quality_status': quality_status,
            'field_scores': field_scores,
            'problematic_fields': problematic_fields,
            'total_fields': total_fields,
            'valid_fields_count': sum(1 for score in field_scores.values() if score >= 0.7),
            'critical_issues_count': len([r for r in validation_results if r.level == ValidationLevel.CRITICAL and not r.is_valid]),
            'warning_issues_count': len([r for r in validation_results if r.level == ValidationLevel.WARNING and not r.is_valid]),
            'recommendation': self._generate_recommendation(quality_status, problematic_fields)
        }
    
    def _calculate_field_score(self, field_name: str, 
                             results: List[ValidationResult]) -> Tuple[float, Dict]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ ÛŒÚ© ÙÛŒÙ„Ø¯ Ø®Ø§Øµ"""
        if not results:
            return 1.0, {}  # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯Ù‡ØŒ Ø§Ù…ØªÛŒØ§Ø² Ú©Ø§Ù…Ù„
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ø¯ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙÛŒÙ„Ø¯
        worst_result = max(results, key=lambda x: self.error_weights[x.level])
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø·Ø­ Ø®Ø·Ø§
        base_score = 1.0 - (self.error_weights[worst_result.level] * 0.1)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø§Ù‡Ù…ÛŒØª ÙÛŒÙ„Ø¯
        importance = self.field_importance.get(field_name, 0.5)
        adjusted_score = max(0.0, min(1.0, base_score * (0.5 + importance * 0.5)))
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø´Ú©Ù„
        issue_info = {
            'level': worst_result.level.value,
            'message': worst_result.message,
            'validation_type': worst_result.validation_type
        }
        
        return adjusted_score, issue_info
    
    def _calculate_overall_score(self, field_scores: Dict[str, float], 
                               total_fields: int) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ Ø±Ú©ÙˆØ±Ø¯"""
        if not field_scores:
            return 0.0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª ÙÛŒÙ„Ø¯Ù‡Ø§
        weighted_sum = 0.0
        total_weight = 0.0
        
        for field_name, score in field_scores.items():
            weight = self.field_importance.get(field_name, 0.5)
            weighted_sum += score * weight
            total_weight += weight
        
        # Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        if total_weight > 0:
            base_score = weighted_sum / total_weight
        else:
            base_score = sum(field_scores.values()) / len(field_scores)
        
        # Ø¬Ø±ÛŒÙ…Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯Ù‡
        missing_fields_penalty = (total_fields - len(field_scores)) / total_fields * 0.3
        
        final_score = max(0.0, base_score - missing_fields_penalty)
        
        return round(final_score, 3)
    
    def _determine_quality_status(self, score: float) -> str:
        """ØªØ¹ÛŒÛŒÙ† ÙˆØ¶Ø¹ÛŒØª Ú©ÛŒÙÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø²"""
        for status, threshold in self.quality_thresholds.items():
            if score >= threshold:
                return status
        return "CRITICAL"
    
    def _identify_problematic_fields(self, field_scores: Dict[str, float],
                                   field_issues: Dict[str, Dict]) -> List[Dict]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±"""
        problematic = []
        
        for field_name, score in field_scores.items():
            if score < 0.7:  # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù…ØªØ± Ø§Ø² 70%
                issue = field_issues.get(field_name, {})
                problematic.append({
                    'field_name': field_name,
                    'score': score,
                    'issue_level': issue.get('level', 'UNKNOWN'),
                    'issue_message': issue.get('message', 'Ù…Ø´Ú©Ù„ Ù†Ø§Ù…Ø´Ø®Øµ'),
                    'importance': self.field_importance.get(field_name, 0.5)
                })
        
        # Ù…Ø±ØªØ¨ Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª Ùˆ Ø§Ù…ØªÛŒØ§Ø²
        problematic.sort(key=lambda x: (x['importance'], -x['score']), reverse=True)
        
        return problematic
    
    def _generate_recommendation(self, quality_status: str, 
                               problematic_fields: List[Dict]) -> str:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ¶Ø¹ÛŒØª Ú©ÛŒÙÛŒØª"""
        recommendations = {
            'EXCELLENT': "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ú©ÛŒÙÛŒØª Ø¹Ø§Ù„ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø±Ù†Ø¯. Ø§Ø¯Ø§Ù…Ù‡ Ù†Ø¸Ø§Ø±Øª ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.",
            'GOOD': "Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø®ÙˆØ¨ Ø§Ø³Øª. Ù†Ø¸Ø§Ø±Øª Ù…Ø¹Ù…ÙˆÙ„ Ø§Ø¯Ø§Ù…Ù‡ ÛŒØ§Ø¨Ø¯.",
            'ACCEPTABLE': "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ù‡Ø³ØªÙ†Ø¯ Ø§Ù…Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙˆØ¬Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø±Ù†Ø¯.",
            'POOR': "Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¶Ø¹ÛŒÙ Ø§Ø³Øª. Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ ÙÙˆØ±ÛŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.",
            'CRITICAL': "ÙˆØ¶Ø¹ÛŒØª Ø¨Ø­Ø±Ø§Ù†ÛŒ! ØªÙˆÙ‚Ù Ø¹Ù…Ù„ÛŒØ§Øª Ùˆ Ø¨Ø±Ø±Ø³ÛŒ ÙÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª."
        }
        
        base_recommendation = recommendations.get(quality_status, "ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ù…Ø´Ø®Øµ")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
        if problematic_fields:
            critical_fields = [f for f in problematic_fields if f['issue_level'] == 'CRITICAL']
            if critical_fields:
                field_names = [f['field_name'] for f in critical_fields[:3]]  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 ÙÛŒÙ„Ø¯
                base_recommendation += f" ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {', '.join(field_names)}"
        
        return base_recommendation
    
    def _get_empty_score(self) -> Dict[str, Any]:
        """Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù„ÛŒ"""
        return {
            'overall_score': 0.0,
            'quality_status': 'CRITICAL',
            'field_scores': {},
            'problematic_fields': [],
            'total_fields': 0,
            'valid_fields_count': 0,
            'critical_issues_count': 0,
            'warning_issues_count': 0,
            'recommendation': "Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯"
        }

# ================================
# Ø´Ù…Ø§Ø±Ù‡ Û¹: Ø³ÛŒØ³ØªÙ… Ø§ØµÙ„Ø§Ø­ Ø®ÙˆØ¯Ú©Ø§Ø±
# ================================

class DataAutoCorrector:
    """Ø³ÛŒØ³ØªÙ… Ø§ØµÙ„Ø§Ø­ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø³Ø§Ù„Ù…"""
    
    def __init__(self, correction_strategy: str = "conservative"):
        self.correction_strategy = correction_strategy
        self.correction_log = []
        
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„Ø§Ø­ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù…Ø´Ú©Ù„
        self.correction_methods = {
            "missing_values": self._correct_missing_values,
            "range_validation": self._correct_range_violations,
            "outlier_detection": self._correct_outliers,
            "data_type_validation": self._correct_data_type,
            "temporal_consistency": self._correct_temporal_issues,
            "consistency_check": self._correct_consistency_issues
        }
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ
        self.strategy_params = {
            "conservative": {
                "max_correction_ratio": 0.1,  # Ø­Ø¯Ø§Ú©Ø«Ø± 10% ØªØºÛŒÛŒØ±
                "use_historical": True,
                "confidence_threshold": 0.8
            },
            "moderate": {
                "max_correction_ratio": 0.3,  # Ø­Ø¯Ø§Ú©Ø«Ø± 30% ØªØºÛŒÛŒØ±
                "use_historical": True, 
                "confidence_threshold": 0.6
            },
            "aggressive": {
                "max_correction_ratio": 0.5,  # Ø­Ø¯Ø§Ú©Ø«Ø± 50% ØªØºÛŒÛŒØ±
                "use_historical": False,
                "confidence_threshold": 0.4
            }
        }
    
    def auto_correct_data(self, df: pd.DataFrame, 
                         validation_results: List[ValidationResult],
                         historical_data: pd.DataFrame = None) -> Tuple[pd.DataFrame, List[Dict]]:
        """
        Ø§ØµÙ„Ø§Ø­ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø³Ø§Ù„Ù…
        """
        self.correction_log = []
        corrected_df = df.copy()
        
        if corrected_df.empty:
            return corrected_df, self.correction_log
        
        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø´Ú©Ù„Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„Ø¯ Ùˆ Ù†ÙˆØ¹
        problems_by_field = self._group_problems_by_field(validation_results)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø§ØµÙ„Ø§Ø­Ø§Øª
        for field_name, problems in problems_by_field.items():
            if field_name not in corrected_df.columns:
                continue
                
            for problem in problems:
                corrected_df = self._apply_correction(
                    corrected_df, field_name, problem, historical_data
                )
        
        return corrected_df, self.correction_log
    
    def _group_problems_by_field(self, validation_results: List[ValidationResult]) -> Dict[str, List]:
        """Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø´Ú©Ù„Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„Ø¯"""
        problems_by_field = {}
        
        for result in validation_results:
            if not result.is_valid and result.corrected_value is None:
                if result.field_name not in problems_by_field:
                    problems_by_field[result.field_name] = []
                problems_by_field[result.field_name].append(result)
        
        return problems_by_field
    
    def _apply_correction(self, df: pd.DataFrame, field_name: str, 
                         problem: ValidationResult, historical_data: pd.DataFrame = None) -> pd.DataFrame:
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ØµÙ„Ø§Ø­ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø´Ú©Ù„ Ø®Ø§Øµ"""
        correction_method = self.correction_methods.get(problem.validation_type)
        
        if correction_method:
            try:
                original_value = df[field_name].iloc[0] if not df.empty else problem.original_value
                
                corrected_value, confidence = correction_method(
                    df, field_name, problem, historical_data
                )
                
                if corrected_value is not None and self._should_apply_correction(original_value, corrected_value, confidence):
                    df[field_name] = df[field_name].astype(type(corrected_value))
                    df.loc[df.index, field_name] = corrected_value
                    
                    # Ø«Ø¨Øª Ø¯Ø± Ù„Ø§Ú¯
                    self._log_correction(field_name, original_value, corrected_value, 
                                       problem.validation_type, confidence, problem.message)
                
            except Exception as e:
                warnings.warn(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØµÙ„Ø§Ø­ ÙÛŒÙ„Ø¯ {field_name}: {e}")
        
        return df
    
    def _correct_missing_values(self, df: pd.DataFrame, field_name: str,
                               problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡"""
        strategy_params = self.strategy_params[self.correction_strategy]
        
        # Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        correction_methods = [
            self._impute_with_forward_fill,
            self._impute_with_historical_mean,
            self._impute_with_knn,
            self._impute_with_median
        ]
        
        best_value = None
        best_confidence = 0.0
        
        for method in correction_methods:
            try:
                value, confidence = method(df, field_name, historical_data)
                if confidence > best_confidence and confidence >= strategy_params["confidence_threshold"]:
                    best_value = value
                    best_confidence = confidence
            except Exception:
                continue
        
        return best_value, best_confidence
    
    def _impute_with_forward_fill(self, df: pd.DataFrame, field_name: str, 
                                 historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± Ø±Ú©ÙˆØ±Ø¯ Ù‚Ø¨Ù„ÛŒ"""
        if len(df) < 2:
            return None, 0.0
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² forward fill
        filled_series = df[field_name].fillna(method='ffill')
        if not filled_series.isna().all():
            return filled_series.iloc[-1], 0.7
        return None, 0.0
    
    def _impute_with_historical_mean(self, df: pd.DataFrame, field_name: str,
                                   historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¨Ø§ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ§Ø±ÛŒØ®ÛŒ"""
        if historical_data is not None and field_name in historical_data.columns:
            historical_mean = historical_data[field_name].mean()
            if not pd.isna(historical_mean):
                return historical_mean, 0.8
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø®ÙˆØ¯ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
        current_mean = df[field_name].mean()
        if not pd.isna(current_mean):
            return current_mean, 0.6
        
        return None, 0.0
    
    def _impute_with_median(self, df: pd.DataFrame, field_name: str,
                           historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¨Ø§ Ù…ÛŒØ§Ù†Ù‡"""
        median_value = df[field_name].median()
        if not pd.isna(median_value):
            return median_value, 0.75
        return None, 0.0
    
    def _correct_range_violations(self, df: pd.DataFrame, field_name: str,
                                 problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡"""
        # Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø­ÙØ§Ø±ÛŒ
        standard_ranges = {
            'WOB': (0, 50000), 'RPM': (0, 200), 'Torque': (0, 50000), 'ROP': (0, 100),
            'Standpipe_Pressure': (0, 50000000), 'Mud_Flow_Rate': (0, 2000),
            'Mud_Temperature': (0, 150), 'Gamma_Ray': (0, 200), 'Resistivity': (0, 1000),
            'Density': (1.5, 3.0), 'Porosity': (0, 50), 'Hook_Load': (0, 1000000),
            'Vibration': (0, 100), 'Bit_Wear': (0, 1), 'Pump_Efficiency': (0, 1)
        }
        
        if field_name not in standard_ranges:
            return None, 0.0
        
        min_val, max_val = standard_ranges[field_name]
        current_value = df[field_name].iloc[-1] if not df.empty else problem.original_value
        
        if pd.isna(current_value):
            return None, 0.0
        
        # Ø§ØµÙ„Ø§Ø­ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ
        strategy = self.correction_strategy
        if strategy == "conservative":
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù‚Ø¯Ø§Ø± Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ù…Ø±Ø²
            if current_value < min_val:
                corrected = min_val * 1.01  # Ú©Ù…ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø­Ø¯ Ù¾Ø§ÛŒÛŒÙ†
            else:
                corrected = max_val * 0.99  # Ú©Ù…ÛŒ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² Ø­Ø¯ Ø¨Ø§Ù„Ø§
            confidence = 0.9
        
        elif strategy == "moderate":
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÛŒØ§Ù†Ù‡ Ù…Ø­Ø¯ÙˆØ¯Ù‡
            corrected = (min_val + max_val) / 2
            confidence = 0.7
        
        else:  # aggressive
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ§Ø±ÛŒØ®ÛŒ ÛŒØ§ Ù…ÛŒØ§Ù†Ù‡
            if historical_data is not None and field_name in historical_data.columns:
                corrected = historical_data[field_name].mean()
            else:
                corrected = df[field_name].median()
            confidence = 0.6
        
        return np.clip(corrected, min_val, max_val), confidence
    
    def _correct_outliers(self, df: pd.DataFrame, field_name: str,
                         problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª"""
        if len(df) < 3:
            return None, 0.0
        
        current_value = df[field_name].iloc[-1]
        values = df[field_name].dropna()
        
        if len(values) < 3:
            return None, 0.0
        
        # Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ø§ØµÙ„Ø§Ø­ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª
        correction_methods = [
            self._correct_outlier_with_iqr,
            self._correct_outlier_with_winsorization,
            self._correct_outlier_with_rolling_median
        ]
        
        best_value = None
        best_confidence = 0.0
        
        for method in correction_methods:
            try:
                value, confidence = method(values, current_value)
                if confidence > best_confidence:
                    best_value = value
                    best_confidence = confidence
            except Exception:
                continue
        
        return best_value, best_confidence
    
    def _correct_outlier_with_iqr(self, values: pd.Series, current_value: float) -> Tuple[float, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù†Ù‚Ø·Ù‡ Ù¾Ø±Øª Ø¨Ø§ Ø±ÙˆØ´ IQR"""
        Q1 = values.quantile(0.25)
        Q3 = values.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        if current_value < lower_bound:
            return lower_bound, 0.8
        elif current_value > upper_bound:
            return upper_bound, 0.8
        else:
            return current_value, 1.0
    
    def _correct_outlier_with_winsorization(self, values: pd.Series, current_value: float) -> Tuple[float, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù†Ù‚Ø·Ù‡ Ù¾Ø±Øª Ø¨Ø§ Winsorization"""
        lower_limit = values.quantile(0.05)
        upper_limit = values.quantile(0.95)
        
        corrected = np.clip(current_value, lower_limit, upper_limit)
        confidence = 0.75 if corrected != current_value else 1.0
        
        return corrected, confidence
    
    def _correct_outlier_with_rolling_median(self, values: pd.Series, current_value: float) -> Tuple[float, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù†Ù‚Ø·Ù‡ Ù¾Ø±Øª Ø¨Ø§ Ù…ÛŒØ§Ù†Ù‡ Ù…ØªØ­Ø±Ú©"""
        if len(values) >= 5:
            rolling_median = values.rolling(window=5, min_periods=1).median().iloc[-1]
            return rolling_median, 0.7
        else:
            median_value = values.median()
            return median_value, 0.6
    def _correct_temporal_issues(self, df: pd.DataFrame, field_name: str,
                            problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù…Ø´Ú©Ù„Ø§Øª Ø²Ù…Ø§Ù†ÛŒ - Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡"""
        if len(df) < 2:
            return None, 0.0
        median_value = df[field_name].median()
        return median_value, 0.6
    def _correct_consistency_issues(self, df: pd.DataFrame, field_name: str,
                              problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù…Ø´Ú©Ù„Ø§Øª Ù†Ø§Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ù…Ù†Ø·Ù‚ÛŒ"""
        if field_name == 'Bit_Wear' and 'ÙØ±Ø³Ø§ÛŒØ´ Ù…ØªÙ‡' in problem.message:
            # Ø§ØµÙ„Ø§Ø­ ÙØ±Ø³Ø§ÛŒØ´ Ù…ØªÙ‡ Ø¨ÛŒØ´ Ø§Ø² 100%
            return 1.0, 0.9
        elif 'Ø¹Ù…Ù‚' in problem.message or 'Depth' in problem.message:
            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú©Ø§Ù‡Ø´ Ø¹Ù…Ù‚
            if len(df) > 1:
                last_valid_depth = df[df['Depth'].diff() >= 0]['Depth'].iloc[-1]
                return last_valid_depth + 0.1, 0.8
            return None, 0.0
    def _correct_data_type(self, df: pd.DataFrame, field_name: str,
                            problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡"""
        current_value = df[field_name].iloc[-1]
        try:
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±
            if field_name in ['WOB', 'RPM', 'Torque', 'ROP', 'Standpipe_Pressure', 
                            'Mud_Flow_Rate', 'Mud_Temperature', 'Gamma_Ray', 
                            'Resistivity', 'Density', 'Porosity', 'Hook_Load', 
                            'Vibration', 'Bit_Wear', 'Pump_Efficiency']:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
                corrected = float(current_value)
                confidence = 0.9
            
            elif field_name in ['Maintenance_Flag', 'Emergency_Stop']:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¨ÙˆÙ„ÛŒÙ†
                if str(current_value).lower() in ['true', '1', 'yes', 'y']:
                    corrected = 1
                else:
                    corrected = 0
                confidence = 0.8
            
            else:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ - Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¬Ø§Ø²
                corrected = str(current_value).strip()
                confidence = 0.7
                
            return corrected, confidence
        
        except (ValueError, TypeError):
            return None, 0.0
    def run_system_test():
        try:
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±
            if field_name in ['WOB', 'RPM', 'Torque', 'ROP', 'Standpipe_Pressure', 
                            'Mud_Flow_Rate', 'Mud_Temperature', 'Gamma_Ray', 
                            'Resistivity', 'Density', 'Porosity', 'Hook_Load', 
                            'Vibration', 'Bit_Wear', 'Pump_Efficiency']:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
                corrected = float(current_value)
                confidence = 0.9
            
            elif field_name in ['Maintenance_Flag', 'Emergency_Stop']:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¨ÙˆÙ„ÛŒÙ†
                if str(current_value).lower() in ['true', '1', 'yes', 'y']:
                    corrected = 1
                else:
                    corrected = 0
                confidence = 0.8
            
            else:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ - Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¬Ø§Ø²
                corrected = str(current_value).strip()
                confidence = 0.7
                
            return corrected, confidence
            
        except (ValueError, TypeError):
            return None, 0.0

    def _should_apply_correction(self, original_value: Any, corrected_value: Any, 
                                confidence: float) -> bool:
        """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§Ø¹Ù…Ø§Ù„ Ø§ØµÙ„Ø§Ø­"""
        if corrected_value is None:
            return False
        
        strategy_params = self.strategy_params[self.correction_strategy]
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        if confidence < strategy_params["confidence_threshold"]:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒØ²Ø§Ù† ØªØºÛŒÛŒØ±
        try:
            if isinstance(original_value, (int, float)) and isinstance(corrected_value, (int, float)):
                if original_value == 0:
                    change_ratio = abs(corrected_value)
                else:
                    change_ratio = abs(corrected_value - original_value) / abs(original_value)
                
                if change_ratio > strategy_params["max_correction_ratio"]:
                    return False
        except (TypeError, ZeroDivisionError):
            pass
        
        return True

    def _log_correction(self, field_name: str, original_value: Any, corrected_value: Any,
                        validation_type: str, confidence: float, problem_message: str):
        """Ø«Ø¨Øª Ù„Ø§Ú¯ Ø§ØµÙ„Ø§Ø­Ø§Øª"""
        correction_record = {
            'timestamp': datetime.now(),
            'field_name': field_name,
            'original_value': original_value,
            'corrected_value': corrected_value,
            'validation_type': validation_type,
            'confidence': confidence,
            'problem_message': problem_message,
            'correction_strategy': self.correction_strategy
        }
        
        self.correction_log.append(correction_record)

    def get_correction_summary(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø§ØµÙ„Ø§Ø­Ø§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡"""
        if not self.correction_log:
            return {
                'total_corrections': 0,
                'fields_corrected': [],
                'average_confidence': 0.0,
                'correction_strategy': self.correction_strategy
            }
        
        fields_corrected = list(set(log['field_name'] for log in self.correction_log))
        total_corrections = len(self.correction_log)
        avg_confidence = np.mean([log['confidence'] for log in self.correction_log])
        
        return {
            'total_corrections': total_corrections,
            'fields_corrected': fields_corrected,
            'average_confidence': round(avg_confidence, 3),
            'correction_strategy': self.correction_strategy,
            'correction_details': self.correction_log
        }

    def _correct_temporal_issues(self, df: pd.DataFrame, field_name: str,
                            problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù…Ø´Ú©Ù„Ø§Øª Ø²Ù…Ø§Ù†ÛŒ - Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡"""
        return None, 0.0

    def _correct_data_type(self, df: pd.DataFrame, field_name: str,
                        problem: ValidationResult, historical_data: pd.DataFrame = None) -> Tuple[Any, float]:
        """Ø§ØµÙ„Ø§Ø­ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡"""
        current_value = df[field_name].iloc[-1]
        
        try:
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±
            if field_name in ['WOB', 'RPM', 'Torque', 'ROP', 'Standpipe_Pressure', 
                            'Mud_Flow_Rate', 'Mud_Temperature', 'Gamma_Ray', 
                            'Resistivity', 'Density', 'Porosity', 'Hook_Load', 
                            'Vibration', 'Bit_Wear', 'Pump_Efficiency']:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
                corrected = float(current_value)
                confidence = 0.9
            
            elif field_name in ['Maintenance_Flag', 'Emergency_Stop']:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¨ÙˆÙ„ÛŒÙ†
                if str(current_value).lower() in ['true', '1', 'yes', 'y']:
                    corrected = 1
                else:
                    corrected = 0
                confidence = 0.8
            
            else:
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ - Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¬Ø§Ø²
                corrected = str(current_value).strip()
                confidence = 0.7
                
            return corrected, confidence
            
        except (ValueError, TypeError):
            return None, 0.0

    def _should_apply_correction(self, original_value: Any, corrected_value: Any, 
                            confidence: float) -> bool:
        """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§Ø¹Ù…Ø§Ù„ Ø§ØµÙ„Ø§Ø­"""
        if corrected_value is None:
            return False
        
        strategy_params = self.strategy_params[self.correction_strategy]
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        if confidence < strategy_params["confidence_threshold"]:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒØ²Ø§Ù† ØªØºÛŒÛŒØ±
        try:
            if isinstance(original_value, (int, float)) and isinstance(corrected_value, (int, float)):
                if original_value == 0:
                    change_ratio = abs(corrected_value)
                else:
                    change_ratio = abs(corrected_value - original_value) / abs(original_value)
                
                if change_ratio > strategy_params["max_correction_ratio"]:
                    return False
        except (TypeError, ZeroDivisionError):
            pass
        
        return True

    def _log_correction(self, field_name: str, original_value: Any, corrected_value: Any,
                    validation_type: str, confidence: float, problem_message: str):
        """Ø«Ø¨Øª Ù„Ø§Ú¯ Ø§ØµÙ„Ø§Ø­Ø§Øª"""
        correction_record = {
            'timestamp': datetime.now(),
            'field_name': field_name,
            'original_value': original_value,
            'corrected_value': corrected_value,
            'validation_type': validation_type,
            'confidence': confidence,
            'problem_message': problem_message,
            'correction_strategy': self.correction_strategy
        }
        
        self.correction_log.append(correction_record)

    def get_correction_summary(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø§ØµÙ„Ø§Ø­Ø§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡"""
        if not self.correction_log:
            return {
                'total_corrections': 0,
                'fields_corrected': [],
                'average_confidence': 0.0,
                'correction_strategy': self.correction_strategy
            }
        
        fields_corrected = list(set(log['field_name'] for log in self.correction_log))
        total_corrections = len(self.correction_log)
        avg_confidence = np.mean([log['confidence'] for log in self.correction_log])
        
        return {
            'total_corrections': total_corrections,
            'fields_corrected': fields_corrected,
            'average_confidence': round(avg_confidence, 3),
            'correction_strategy': self.correction_strategy,
            'correction_details': self.correction_log
        }

    # ================================
    # Ø´Ù…Ø§Ø±Ù‡ Û±Û°: Ø³ÛŒØ³ØªÙ… Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ
    # ================================

class ReportGenerator:
    """Ø³ÛŒØ³ØªÙ… ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ"""
    
    def __init__(self, validator_version: str = "1.0.0"):
        self.validator_version = validator_version
    
    def generate_quality_report(self, 
                            df: pd.DataFrame,
                            validation_results: List[ValidationResult],
                            quality_scores: Dict[str, Any],
                            correction_log: List[Dict] = None,
                            report_level: ReportLevel = ReportLevel.DETAILED,
                            data_source: str = "Unknown") -> DataQualityReport:
        """
        ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡
        """
        start_time = datetime.now()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        stats = self._calculate_statistics(validation_results, quality_scores, correction_log)
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
        report = DataQualityReport(
            report_id=f"DQR_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            generation_time=datetime.now(),
            data_source=data_source,
            total_records=len(df),
            total_fields=len(df.columns) if not df.empty else 0,
            time_range=(
                df['Timestamp'].min() if 'Timestamp' in df.columns else datetime.now(),
                df['Timestamp'].max() if 'Timestamp' in df.columns else datetime.now()
            ),
            overall_quality_score=quality_scores.get('overall_score', 0),
            quality_status=quality_scores.get('quality_status', 'UNKNOWN'),
            validation_duration=(datetime.now() - start_time).total_seconds(),
            total_issues=stats['total_issues'],
            critical_issues=stats['critical_issues'],
            warning_issues=stats['warning_issues'],
            info_issues=stats['info_issues'],
            issues_by_type=stats['issues_by_type'],
            issues_by_field=stats['issues_by_field'],
            issues_by_severity=stats['issues_by_severity'],
            total_corrections=stats['total_corrections'],
            correction_success_rate=stats['correction_success_rate'],
            corrected_fields=stats['corrected_fields'],
            recommendations=self._generate_recommendations(stats, quality_scores),
            priority_actions=self._generate_priority_actions(stats, quality_scores),
            validation_results=validation_results,
            correction_log=correction_log or [],
            quality_scores=quality_scores,
            report_level=report_level,
            validator_version=self.validator_version
        )
        
        return report
    
    def _calculate_statistics(self, validation_results: List[ValidationResult],
                            quality_scores: Dict[str, Any],
                            correction_log: List[Dict]) -> Dict[str, Any]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ùˆ Ø§Ø±Ù‚Ø§Ù… Ú¯Ø²Ø§Ø±Ø´"""
        
        # Ø¢Ù…Ø§Ø± Ø®Ø·Ø§Ù‡Ø§
        critical_issues = len([r for r in validation_results if r.level == ValidationLevel.CRITICAL and not r.is_valid])
        warning_issues = len([r for r in validation_results if r.level == ValidationLevel.WARNING and not r.is_valid])
        info_issues = len([r for r in validation_results if r.level == ValidationLevel.INFO and not r.is_valid])
        total_issues = critical_issues + warning_issues + info_issues
        
        # ØªÙˆØ²ÛŒØ¹ Ø®Ø·Ø§Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
        issues_by_type = {}
        for result in validation_results:
            if not result.is_valid:
                issues_by_type[result.validation_type] = issues_by_type.get(result.validation_type, 0) + 1
        
        # ØªÙˆØ²ÛŒØ¹ Ø®Ø·Ø§Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„Ø¯
        issues_by_field = {}
        for result in validation_results:
            if not result.is_valid:
                issues_by_field[result.field_name] = issues_by_field.get(result.field_name, 0) + 1
        
        # ØªÙˆØ²ÛŒØ¹ Ø®Ø·Ø§Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¯Øª
        issues_by_severity = {
            'CRITICAL': critical_issues,
            'WARNING': warning_issues,
            'INFO': info_issues
        }
        
        # Ø¢Ù…Ø§Ø± Ø§ØµÙ„Ø§Ø­Ø§Øª
        total_corrections = len(correction_log) if correction_log else 0
        successful_corrections = len([log for log in (correction_log or []) 
                                    if log.get('confidence', 0) >= 0.5])
        correction_success_rate = (successful_corrections / total_corrections * 100) if total_corrections > 0 else 0
        
        corrected_fields = list(set(log['field_name'] for log in (correction_log or [])))
        
        return {
            'total_issues': total_issues,
            'critical_issues': critical_issues,
            'warning_issues': warning_issues,
            'info_issues': info_issues,
            'issues_by_type': issues_by_type,
            'issues_by_field': issues_by_field,
            'issues_by_severity': issues_by_severity,
            'total_corrections': total_corrections,
            'correction_success_rate': correction_success_rate,
            'corrected_fields': corrected_fields
        }
    
    def _generate_recommendations(self, stats: Dict[str, Any], 
                                quality_scores: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª"""
        recommendations = []
        
        overall_score = quality_scores.get('overall_score', 0)
        critical_issues = stats['critical_issues']
        warning_issues = stats['warning_issues']
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ
        if overall_score >= 0.9:
            recommendations.append("Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª. Ø§Ø¯Ø§Ù…Ù‡ Ù†Ø¸Ø§Ø±Øª Ø¨Ø§ ÙØ±Ú©Ø§Ù†Ø³ ÙØ¹Ù„ÛŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        elif overall_score >= 0.8:
            recommendations.append("Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø®ÙˆØ¨ Ø§Ø³Øª. Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        elif overall_score >= 0.7:
            recommendations.append("Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø§Ø³Øª. Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.")
        elif overall_score >= 0.6:
            recommendations.append("Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¶Ø¹ÛŒÙ Ø§Ø³Øª. Ø§Ù‚Ø¯Ø§Ù… ÙÙˆØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.")
        else:
            recommendations.append("ÙˆØ¶Ø¹ÛŒØª Ø¨Ø­Ø±Ø§Ù†ÛŒ! ØªÙˆÙ‚Ù Ø¹Ù…Ù„ÛŒØ§Øª Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.")
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù†ÙˆØ¹ Ø®Ø·Ø§
        issues_by_type = stats['issues_by_type']
        
        if issues_by_type.get('missing_values', 0) > 0:
            recommendations.append(f"ØªØ¹Ø¯Ø§Ø¯ {issues_by_type['missing_values']} Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ Ùˆ Ø³ÛŒØ³ØªÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        
        if issues_by_type.get('range_validation', 0) > 0:
            recommendations.append("Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡. Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ† Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.")
        
        if issues_by_type.get('outlier_detection', 0) > 0:
            recommendations.append("Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡. Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø§ÛŒØ· Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ùˆ ØµØ­Øª Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        
        return recommendations
    
    def _generate_priority_actions(self, stats: Dict[str, Any],
                                quality_scores: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ø§Ø±"""
        actions = []
        
        critical_issues = stats['critical_issues']
        problematic_fields = quality_scores.get('problematic_fields', [])
        
        if critical_issues > 0:
            actions.append("ğŸ”´ ØªÙˆÙ‚Ù Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ")
            actions.append("ğŸ”´ Ø¨Ø±Ø±Ø³ÛŒ ÙÙˆØ±ÛŒ Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ")
        
        if stats['issues_by_type'].get('missing_values', 0) > 10:
            actions.append("ğŸŸ  Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ùˆ Ø³Ù„Ø§Ù…Øª Ø³Ù†Ø³ÙˆØ±Ù‡Ø§")
        
        if stats['issues_by_type'].get('range_validation', 0) > 5:
            actions.append("ğŸŸ  Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ† ÙÙˆØ±ÛŒ Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡")
        
        if len(actions) == 0:
            actions.append("âœ… Ø§Ø¯Ø§Ù…Ù‡ Ù†Ø¸Ø§Ø±Øª Ù…Ø¹Ù…ÙˆÙ„ - Ù‡ÛŒÚ† Ø§Ù‚Ø¯Ø§Ù… ÙÙˆØ±ÛŒ Ù„Ø§Ø²Ù… Ù†ÛŒØ³Øª")
        
        return actions

# ================================
# Ø´Ù…Ø§Ø±Ù‡ Û±-Û·: Ø³ÛŒØ³ØªÙ… Ø§ØµÙ„ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
# ================================

class DataQualityValidator:
    """Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­ÙØ§Ø±ÛŒ"""
    
    def __init__(self, historical_data: pd.DataFrame = None, validation_config: Dict = None):
        """
        Args:
            historical_data: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§
            validation_config: Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        """
        self.historical_data = historical_data
        self.config = validation_config or self._get_default_config()
        
        # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML
        self.outlier_detector = None
        self._train_ml_models()
        
        # Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ
        self.scorer = DataQualityScorer()
        self.corrector = DataAutoCorrector(self.config.get('correction_strategy', 'moderate'))
        self.reporter = ReportGenerator()
        
        # Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø­ÙØ§Ø±ÛŒ
        self.standard_ranges = self._get_standard_ranges()
        
        print("âœ… Ø³ÛŒØ³ØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def _get_default_config(self) -> Dict:
        """Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ"""
        return {
            'enable_auto_correction': True,
            'correction_strategy': 'moderate',
            'validation_level': 'detailed',
            'outlier_detection_method': 'auto',
            'missing_values_threshold': 0.2,  # 20%
            'enable_ml_detection': True
        }
    
    def _get_standard_ranges(self) -> Dict[str, Tuple[float, float]]:
        """Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø­ÙØ§Ø±ÛŒ"""
        return {
            'WOB': (0, 50000),                    # ÙˆØ²Ù† Ø±ÙˆÛŒ Ù…ØªÙ‡ (Ù¾ÙˆÙ†Ø¯)
            'RPM': (0, 200),                      # Ø¯ÙˆØ± Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡
            'Torque': (0, 50000),                 # Ú¯Ø´ØªØ§ÙˆØ± (ÙÙˆØª-Ù¾ÙˆÙ†Ø¯)
            'ROP': (0, 100),                      # Ù†Ø±Ø® Ù†ÙÙˆØ° (ÙÙˆØª Ø¯Ø± Ø³Ø§Ø¹Øª)
            'Depth': (0, 50000),                  # Ø¹Ù…Ù‚ (ÙÙˆØª)
            'Standpipe_Pressure': (0, 50000000),  # ÙØ´Ø§Ø± (Ù¾Ø§Ø³Ú©Ø§Ù„)
            'Mud_Flow_Rate': (0, 2000),           # Ù†Ø±Ø® Ø¬Ø±ÛŒØ§Ù† Ú¯Ù„ (Ú¯Ø§Ù„Ù† Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡)
            'Mud_Temperature': (0, 150),          # Ø¯Ù…Ø§ÛŒ Ú¯Ù„ (Ø¯Ø±Ø¬Ù‡ Ø³Ø§Ù†ØªÛŒâ€ŒÚ¯Ø±Ø§Ø¯)
            'Gamma_Ray': (0, 200),                # Ù¾Ø±ØªÙˆÛŒ Ú¯Ø§Ù…Ø§ (API)
            'Resistivity': (0, 1000),             # Ù…Ù‚Ø§ÙˆÙ…Øª (Ø§Ù‡Ù…-Ù…ØªØ±)
            'Density': (1.5, 3.0),                # Ú†Ú¯Ø§Ù„ÛŒ (Ú¯Ø±Ù… Ø¨Ø± Ø³Ø§Ù†ØªÛŒÙ…ØªØ± Ù…Ú©Ø¹Ø¨)
            'Porosity': (0, 50),                  # ØªØ®Ù„Ø®Ù„ (Ø¯Ø±ØµØ¯)
            'Hook_Load': (0, 1000000),            # Ø¨Ø§Ø± Ù‚Ù„Ø§Ø¨ (Ù¾ÙˆÙ†Ø¯)
            'Vibration': (0, 100),                # Ø§Ø±ØªØ¹Ø§Ø´ (g)
            'Bit_Wear': (0, 1),                   # ÙØ±Ø³Ø§ÛŒØ´ Ù…ØªÙ‡ (0-1)
            'Pump_Efficiency': (0, 1),            # Ú©Ø§Ø±Ø§ÛŒÛŒ Ù¾Ù…Ù¾ (0-1)
            'Mud_Weight': (8, 20)                 # ÙˆØ²Ù† Ú¯Ù„ (Ù¾ÙˆÙ†Ø¯ Ø¨Ø± Ú¯Ø§Ù„Ù†)
        }
    
    def _train_ml_models(self):
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†"""
        if self.historical_data is None or not self.config['enable_ml_detection']:
            return
        
        try:
            numeric_columns = self.historical_data.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ØªØ´Ø®ÛŒØµ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª
                self.outlier_detector = IsolationForest(
                    contamination=0.05,
                    random_state=42
                )
                self.outlier_detector.fit(self.historical_data[numeric_columns].fillna(0))
                print("âœ… Ù…Ø¯Ù„ ØªØ´Ø®ÛŒØµ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
        except Exception as e:
            warnings.warn(f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML: {e}")
            self.outlier_detector = None
    
    def validate(self, df: pd.DataFrame, enable_correction: bool = None) -> Dict[str, Any]:
        """
        Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­ÙØ§Ø±ÛŒ
        
        Args:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­ÙØ§Ø±ÛŒ
            enable_correction: ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§ØµÙ„Ø§Ø­ Ø®ÙˆØ¯Ú©Ø§Ø±
            
        Returns:
            Ù†ØªØ§ÛŒØ¬ Ú©Ø§Ù…Ù„ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        """
        if df.empty:
            return self._get_empty_validation_result()
        
        start_time = datetime.now()
        enable_correction = enable_correction if enable_correction is not None else self.config['enable_auto_correction']
        
        print(f"ğŸ” Ø´Ø±ÙˆØ¹ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ {len(df)} Ø±Ú©ÙˆØ±Ø¯...")
        
        # Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒâ€ŒÙ‡Ø§
        all_validation_results = []
        
        # 1. Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        missing_results = self._check_missing_values(df)
        all_validation_results.extend(missing_results)
        
        # 2. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…Ø­Ø¯ÙˆØ¯Ù‡
        range_results = self._validate_ranges(df)
        all_validation_results.extend(range_results)
        
        # 3. Ú©Ø´Ù Ù†Ù‚Ø§Ø· Ù¾Ø±Øª
        outlier_results = self._detect_outliers(df)
        all_validation_results.extend(outlier_results)
        
        # 4. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡
        type_results = self._validate_data_types(df)
        all_validation_results.extend(type_results)
        
        # 5. Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ù…Ù†Ø·Ù‚ÛŒ
        consistency_results = self._check_consistency(df)
        all_validation_results.extend(consistency_results)
        
        # 6. Ø¨Ø±Ø±Ø³ÛŒ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ Ø²Ù…Ø§Ù†ÛŒ
        temporal_results = self._check_temporal_consistency(df)
        all_validation_results.extend(temporal_results)
        
        # 7. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª
        status_results = self._validate_status_fields(df)
        all_validation_results.extend(status_results)
        
            # 8. Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        correlation_results = self._check_advanced_correlations(df)
        all_validation_results.extend(correlation_results)
        
        # 9. ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡  
        advanced_anomaly_results = self._advanced_anomaly_detection(df)
        all_validation_results.extend(advanced_anomaly_results)
        # Ø§ØµÙ„Ø§Ø­ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø± ØµÙˆØ±Øª ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù†)
        corrected_df = df.copy()
        correction_log = []
        
        if enable_correction:
            corrected_df, correction_log = self.corrector.auto_correct_data(
                df, all_validation_results, self.historical_data
            )
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª
        quality_scores = self.scorer.calculate_record_score(
            all_validation_results, len(df.columns)
        )
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
        report = self.reporter.generate_quality_report(
            df=corrected_df,
            validation_results=all_validation_results,
            quality_scores=quality_scores,
            correction_log=correction_log,
            data_source="DRLLING_DATA_VALIDATION"
        )
        
        validation_duration = (datetime.now() - start_time).total_seconds()
        
        print(f"âœ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ø¯Ø± {validation_duration:.2f} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"ğŸ“Š Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª: {quality_scores['overall_score']} ({quality_scores['quality_status']})")
        print(f"âš ï¸  Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {quality_scores['critical_issues_count']}")
        
        return {
            'is_valid': quality_scores['overall_score'] >= 0.7,
            'validation_results': all_validation_results,
            'quality_scores': quality_scores,
            'corrected_data': corrected_df,
            'correction_log': correction_log,
            'quality_report': report,
            'validation_duration': validation_duration,
            'summary': {
                'total_records': len(df),
                'total_fields': len(df.columns),
                'total_issues': len([r for r in all_validation_results if not r.is_valid]),
                'critical_issues': quality_scores['critical_issues_count'],
                'warning_issues': quality_scores['warning_issues_count'],
                'corrections_applied': len(correction_log)
            }
        }
    
    def _check_missing_values(self, df: pd.DataFrame) -> List[ValidationResult]:
        """1. Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡"""
        results = []
        missing_threshold = self.config['missing_values_threshold']
        
        for column in df.columns:
            missing_count = df[column].isna().sum()
            total_count = len(df)
            
            if missing_count > 0:
                missing_percentage = (missing_count / total_count) * 100
                
                # ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±ØµØ¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
                if missing_percentage > missing_threshold * 100:
                    level = ValidationLevel.CRITICAL
                elif missing_percentage > 5:
                    level = ValidationLevel.WARNING
                else:
                    level = ValidationLevel.INFO
                
                results.append(ValidationResult(
                    field_name=column,
                    is_valid=False,
                    validation_type="missing_values",
                    message=f"{missing_percentage:.1f}% Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ ({missing_count} Ø§Ø² {total_count})",
                    level=level,
                    original_value=None
                ))
            else:
                results.append(ValidationResult(
                    field_name=column,
                    is_valid=True,
                    validation_type="missing_values",
                    message="Ù‡ÛŒÚ† Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯",
                    level=ValidationLevel.INFO,
                    original_value=None
                ))
        
        return results
    
    def _validate_ranges(self, df: pd.DataFrame) -> List[ValidationResult]:
        """2. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ±"""
        results = []
        
        for column in df.columns:
            if column not in self.standard_ranges:
                continue
                
            min_val, max_val = self.standard_ranges[column]
            
            for idx, value in df[column].items():
                if pd.isna(value):
                    continue
                    
                try:
                    value_float = float(value)
                    
                    if value_float < min_val or value_float > max_val:
                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
                        corrected_value = np.clip(value_float, min_val, max_val)
                        
                        results.append(ValidationResult(
                            field_name=column,
                            is_valid=False,
                            validation_type="range_validation",
                            message=f"Ù…Ù‚Ø¯Ø§Ø± {value_float} Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø² [{min_val}, {max_val}]",
                            level=ValidationLevel.WARNING,
                            original_value=value_float,
                            corrected_value=corrected_value
                        ))
                    else:
                        results.append(ValidationResult(
                            field_name=column,
                            is_valid=True,
                            validation_type="range_validation",
                            message=f"Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø² [{min_val}, {max_val}]",
                            level=ValidationLevel.INFO,
                            original_value=value_float
                        ))
                        
                except (ValueError, TypeError):
                    results.append(ValidationResult(
                        field_name=column,
                        is_valid=False,
                        validation_type="range_validation",
                        message=f"Ù…Ù‚Ø¯Ø§Ø± ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ: {value}",
                        level=ValidationLevel.CRITICAL,
                        original_value=value
                    ))
        
        return results
    
    def _detect_outliers(self, df: pd.DataFrame) -> List[ValidationResult]:
        """3. Ú©Ø´Ù Ù†Ù‚Ø§Ø· Ù¾Ø±Øª/Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¢Ù…Ø§Ø±ÛŒ"""
        results = []
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for column in numeric_columns:
            if column not in self.standard_ranges:
                continue
                
            values = df[column].dropna()
            if len(values) < 3:
                continue
            
            # Ø±ÙˆØ´ Z-score
            try:
                z_scores = np.abs(stats.zscore(values))
                z_outliers = values[z_scores > 3]
                
                # Ø±ÙˆØ´ IQR
                Q1 = values.quantile(0.25)
                Q3 = values.quantile(0.75)
                IQR = Q3 - Q1
                iqr_outliers = values[(values < (Q1 - 1.5 * IQR)) | (values > (Q3 + 1.5 * IQR))]
                
                # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
                all_outliers = set(z_outliers.index) | set(iqr_outliers.index)
                
                for idx in all_outliers:
                    value = values.loc[idx]
                    median_value = values.median()
                    
                    results.append(ValidationResult(
                        field_name=column,
                        is_valid=False,
                        validation_type="outlier_detection",
                        message=f"Ù†Ù‚Ø·Ù‡ Ù¾Ø±Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {value}",
                        level=ValidationLevel.WARNING,
                        original_value=value,
                        corrected_value=median_value
                    ))
                    
            except Exception as e:
                warnings.warn(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ø¨Ø±Ø§ÛŒ {column}: {e}")
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ ML Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ú†Ù†Ø¯Ù…ØªØºÛŒØ±Ù‡
        if self.outlier_detector is not None and len(numeric_columns) > 0:
            try:
                features = df[numeric_columns].fillna(0)
                outlier_predictions = self.outlier_detector.predict(features)
                
                for idx, prediction in enumerate(outlier_predictions):
                    if prediction == -1:  # Ù†Ù‚Ø·Ù‡ Ù¾Ø±Øª
                        results.append(ValidationResult(
                            field_name="multivariate_outlier",
                            is_valid=False,
                            validation_type="multivariate_outlier_detection",
                            message="Ù†Ù‚Ø·Ù‡ Ù¾Ø±Øª Ú†Ù†Ø¯Ù…ØªØºÛŒØ±Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡",
                            level=ValidationLevel.WARNING,
                            original_value=df.iloc[idx][numeric_columns].to_dict(),
                            confidence=0.8
                        ))
            except Exception as e:
                warnings.warn(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ú†Ù†Ø¯Ù…ØªØºÛŒØ±Ù‡: {e}")
        
        return results
    
    def _validate_data_types(self, df: pd.DataFrame) -> List[ValidationResult]:
        """4. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡"""
        results = []
        
        # ØªØ¹Ø±ÛŒÙ Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙÛŒÙ„Ø¯
        expected_dtypes = {
            'WOB': 'numeric', 'RPM': 'numeric', 'Torque': 'numeric', 'ROP': 'numeric',
            'Depth': 'numeric', 'Standpipe_Pressure': 'numeric', 'Mud_Temperature': 'numeric',
            'Mud_Flow_Rate': 'numeric', 'Gamma_Ray': 'numeric', 'Resistivity': 'numeric',
            'Density': 'numeric', 'Porosity': 'numeric', 'Hook_Load': 'numeric',
            'Vibration': 'numeric', 'Bit_Wear': 'numeric', 'Pump_Efficiency': 'numeric',
            'Formation_Type': 'categorical', 'Lithology': 'categorical', 
            'Layer_Name': 'categorical', 'Active_Events': 'categorical',
            'Failed_Equipment': 'categorical', 'Maintenance_Flag': 'boolean',
            'Abnormal_Condition': 'categorical'
        }
        
        for column in df.columns:
            if column not in expected_dtypes:
                continue
                
            expected_type = expected_dtypes[column]
            
            for idx, value in df[column].items():
                if pd.isna(value):
                    continue
                    
                is_valid = False
                message = ""
                
                if expected_type == 'numeric':
                    try:
                        float(value)
                        is_valid = True
                        message = "Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø¹Ø¯Ø¯ÛŒ Ù…Ø¹ØªØ¨Ø±"
                    except (ValueError, TypeError):
                        is_valid = False
                        message = f"Ø§Ù†ØªØ¸Ø§Ø± Ø¹Ø¯Ø¯ ÙˆÙ„ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯: {type(value).__name__}"
                
                elif expected_type == 'boolean':
                    if value in [0, 1, True, False, '0', '1', 'True', 'False']:
                        is_valid = True
                        message = "Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø¨ÙˆÙ„ÛŒÙ† Ù…Ø¹ØªØ¨Ø±"
                    else:
                        is_valid = False
                        message = f"Ù…Ù‚Ø¯Ø§Ø± Ø¨ÙˆÙ„ÛŒÙ† Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {value}"
                
                elif expected_type == 'categorical':
                    is_valid = True
                    message = "Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù…Ø¹ØªØ¨Ø±"
                
                results.append(ValidationResult(
                    field_name=column,
                    is_valid=is_valid,
                    validation_type="data_type_validation",
                    message=message,
                    level=ValidationLevel.WARNING if not is_valid else ValidationLevel.INFO,
                    original_value=value
                ))
        
        return results
    
    def _check_consistency(self, df: pd.DataFrame) -> List[ValidationResult]:
        """5. Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ù…Ù†Ø·Ù‚ÛŒ Ø¨ÛŒÙ† ÙÛŒÙ„Ø¯Ù‡Ø§"""
        results = []
        
        # ØªØ¹Ø±ÛŒÙ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù†Ø§Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ù…Ù†Ø·Ù‚ÛŒ
        consistency_rules = [
            {
                'name': 'depth_increase',
                'condition': lambda df: df['Depth'].diff() >= 0,
                'fields': ['Depth'],
                'message': 'Ø¹Ù…Ù‚ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯'
            },
            {
                'name': 'pressure_flow_relation', 
                'condition': lambda df: (df['Mud_Flow_Rate'] > 0) == (df['Standpipe_Pressure'] > 0),
                'fields': ['Mud_Flow_Rate', 'Standpipe_Pressure'],
                'message': 'ÙØ´Ø§Ø± Ùˆ Ù†Ø±Ø® Ø¬Ø±ÛŒØ§Ù† Ú¯Ù„ Ø¨Ø§ÛŒØ¯ Ù‡Ù…Ø²Ù…Ø§Ù† ØµÙØ± ÛŒØ§ ØºÛŒØ±ØµÙØ± Ø¨Ø§Ø´Ù†Ø¯'
            },
            {
                'name': 'wob_torque_relation',
                'condition': lambda df: df['Torque'] <= df['WOB'] * 0.5,
                'fields': ['WOB', 'Torque'],
                'message': 'Ú¯Ø´ØªØ§ÙˆØ± Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨ÛŒØ´ Ø§Ø² 50% WOB Ø¨Ø§Ø´Ø¯'
            },
            {
                'name': 'bit_wear_consistency',
                'condition': lambda df: df['Bit_Wear'] <= 1.0,
                'fields': ['Bit_Wear'],
                'message': 'ÙØ±Ø³Ø§ÛŒØ´ Ù…ØªÙ‡ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨ÛŒØ´ Ø§Ø² 100% Ø¨Ø§Ø´Ø¯'
            }
        ]
        
        for rule in consistency_rules:
            try:
                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
                if not all(field in df.columns for field in rule['fields']):
                    continue
                    
                mask = rule['condition'](df)
                inconsistent_indices = mask[~mask].index
                
                for idx in inconsistent_indices:
                    field_values = {field: df.loc[idx, field] for field in rule['fields']}
                    
                    results.append(ValidationResult(
                        field_name=",".join(rule['fields']),
                        is_valid=False,
                        validation_type="consistency_check",
                        message=f"{rule['message']}. Ù…Ù‚Ø§Ø¯ÛŒØ±: {field_values}",
                        level=ValidationLevel.CRITICAL,
                        original_value=field_values
                    ))
                    
            except Exception as e:
                warnings.warn(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø§Ø¹Ø¯Ù‡ {rule['name']}: {e}")
        
        return results
    
    def _check_temporal_consistency(self, df: pd.DataFrame) -> List[ValidationResult]:
        """6. Ø¨Ø±Ø±Ø³ÛŒ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        results = []
        
        if 'Timestamp' not in df.columns or len(df) < 2:
            return results
        
        # Ù…Ø±ØªØ¨ Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
        df_sorted = df.sort_values('Timestamp').copy()
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ
        time_diffs = (df_sorted['Timestamp'] - df_sorted['Timestamp'].shift(1)).dt.total_seconds()
        abnormal_time_gaps = time_diffs[time_diffs > 3600]  # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒØ´ Ø§Ø² 1 Ø³Ø§Ø¹Øª
        
        for idx, gap in abnormal_time_gaps.items():
            results.append(ValidationResult(
                field_name="Timestamp",
                is_valid=False,
                validation_type="temporal_consistency",
                message=f"ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ: {gap} Ø«Ø§Ù†ÛŒÙ‡ Ø¨ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§",
                level=ValidationLevel.WARNING,
                original_value=df_sorted.loc[idx, 'Timestamp']
            ))
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¬Ù‡Ø´â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ú¯Ù‡Ø§Ù†ÛŒ Ø¯Ø± Ø¹Ù…Ù‚
        if 'Depth' in df_sorted.columns:
            depth_diff = df_sorted['Depth'].diff()
            depth_jumps = depth_diff[(depth_diff.abs() > 100) | (depth_diff < -10)]
            
            for idx, jump in depth_jumps.items():
                if jump < -10:
                    level = ValidationLevel.CRITICAL
                    message = f"Ú©Ø§Ù‡Ø´ ØºÛŒØ±Ù…Ù…Ú©Ù† Ø¯Ø± Ø¹Ù…Ù‚: {jump:.2f} ÙÙˆØª"
                else:
                    level = ValidationLevel.WARNING  
                    message = f"Ø¬Ù‡Ø´ Ù†Ø§Ú¯Ù‡Ø§Ù†ÛŒ Ø¯Ø± Ø¹Ù…Ù‚: {jump:.2f} ÙÙˆØª"
                
                results.append(ValidationResult(
                    field_name="Depth",
                    is_valid=False,
                    validation_type="temporal_consistency",
                    message=message,
                    level=level,
                    original_value=df_sorted.loc[idx, 'Depth']
                ))
        
        return results
    
    def _validate_status_fields(self, df: pd.DataFrame) -> List[ValidationResult]:
        """7. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª"""
        results = []
        
        status_fields = {
            'Maintenance_Flag': [0, 1, True, False],
            'Pump_Efficiency': (0, 1),
            'Bit_Wear': (0, 1),
            'Emergency_Stop': [0, 1, True, False]
        }
        
        for field, allowed_values in status_fields.items():
            if field not in df.columns:
                continue
                
            for idx, value in df[field].items():
                if pd.isna(value):
                    continue
                    
                is_valid = False
                message = ""
                
                if isinstance(allowed_values, (list, tuple)) and isinstance(allowed_values[0], (int, float, bool, str)):
                    if value in allowed_values:
                        is_valid = True
                        message = f"Ù…Ù‚Ø¯Ø§Ø± ÙˆØ¶Ø¹ÛŒØª Ù…Ø¹ØªØ¨Ø±: {value}"
                    else:
                        is_valid = False
                        message = f"Ù…Ù‚Ø¯Ø§Ø± ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {value}. Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¬Ø§Ø²: {allowed_values}"
                        
                elif isinstance(allowed_values, tuple) and len(allowed_values) == 2:
                    try:
                        value_float = float(value)
                        min_val, max_val = allowed_values
                        if min_val <= value_float <= max_val:
                            is_valid = True
                            message = f"Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø² [{min_val}, {max_val}]"
                        else:
                            is_valid = False
                            message = f"Ù…Ù‚Ø¯Ø§Ø± Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø²: {value_float}. Ù…Ø­Ø¯ÙˆØ¯Ù‡: [{min_val}, {max_val}]"
                    except (ValueError, TypeError):
                        is_valid = False
                        message = f"Ù…Ù‚Ø¯Ø§Ø± ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ø¯ ÙˆØ¶Ø¹ÛŒØª: {value}"
                
                level = ValidationLevel.CRITICAL if not is_valid and field in ['Emergency_Stop'] else ValidationLevel.WARNING
                
                results.append(ValidationResult(
                    field_name=field,
                    is_valid=is_valid,
                    validation_type="status_field_validation",
                    message=message,
                    level=level,
                    original_value=value
                ))
        
        return results
    def _check_advanced_correlations(self, df: pd.DataFrame) -> List[ValidationResult]:
        """Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§"""
        results = []
        
        # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¨ÛŒØ´ Ø§Ø² 2 Ø±Ú©ÙˆØ±Ø¯
        if len(df) < 2:
            return results
        
        # Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ WOB Ùˆ Torque
        if 'WOB' in df.columns and 'Torque' in df.columns:
            try:
                correlation = df['WOB'].corr(df['Torque'])
                if not pd.isna(correlation) and correlation < 0.7:
                    results.append(ValidationResult(
                        field_name="WOB-Torque",
                        is_valid=False,
                        validation_type="correlation_check",
                        message=f"Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¶Ø¹ÛŒÙ Ø¨ÛŒÙ† WOB Ùˆ Torque: {correlation:.2f}",
                        level=ValidationLevel.WARNING
                    ))
            except:
                pass
        
        # Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ RPM Ùˆ ROP
        if 'RPM' in df.columns and 'ROP' in df.columns:
            try:
                correlation = df['RPM'].corr(df['ROP'])
                if not pd.isna(correlation) and correlation < 0.5:
                    results.append(ValidationResult(
                        field_name="RPM-ROP", 
                        is_valid=False,
                        validation_type="correlation_check",
                        message=f"Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ Ø¨ÛŒÙ† RPM Ùˆ ROP: {correlation:.2f}",
                        level=ValidationLevel.WARNING
                    ))
            except:
                pass
        
        return results

    def _advanced_anomaly_detection(self, df: pd.DataFrame) -> List[ValidationResult]:
        """Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ"""
        results = []
        
        # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¨ÛŒØ´ Ø§Ø² 5 Ø±Ú©ÙˆØ±Ø¯ Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
        if len(df) < 5:
            return results  # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© Ú©Ø§Ø±ÛŒ Ù†Ú©Ù†
        
        try:
            from sklearn.neighbors import LocalOutlierFactor
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LOF Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                # ØªÙ†Ø¸ÛŒÙ… n_neighbors Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
                n_neighbors = min(20, len(df) - 1)
                lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1)
                outlier_labels = lof.fit_predict(df[numeric_cols].fillna(0))
                
                for idx, label in enumerate(outlier_labels):
                    if label == -1:  # Ù†Ù‚Ø·Ù‡ Ù¾Ø±Øª
                        results.append(ValidationResult(
                            field_name="multivariate_lof",
                            is_valid=False,
                            validation_type="advanced_anomaly_detection", 
                            message="Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ú†Ù†Ø¯Ù…ØªØºÛŒØ±Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ (LOF)",
                            level=ValidationLevel.WARNING,
                            original_value=df.iloc[idx][numeric_cols].to_dict()
                        ))
        except Exception as e:
            # Ø§Ú¯Ø± Ø®Ø·Ø§ Ø¯Ø§Ø¯ØŒ Ø¨ÛŒâ€ŒØ®ÙˆØ¯Ø´ Ú©Ù†
            pass
        
        return results  
    def _get_empty_validation_result(self) -> Dict[str, Any]:
        """Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù„ÛŒ"""
        return {
            'is_valid': False,
            'validation_results': [],
            'quality_scores': {'overall_score': 0.0, 'quality_status': 'CRITICAL'},
            'corrected_data': pd.DataFrame(),
            'correction_log': [],
            'quality_report': None,
            'validation_duration': 0.0,
            'summary': {
                'total_records': 0,
                'total_fields': 0,
                'total_issues': 0,
                'critical_issues': 0,
                'warning_issues': 0,
                'corrections_applied': 0
            }
        }

# ================================
# ØªØ³Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…
# ================================
    def run_system_test():
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª Ø³ÛŒØ³ØªÙ…"""
        print("ğŸš€ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ...")
        print("=" * 50)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª
        test_data = pd.DataFrame({
            'Timestamp': [pd.Timestamp('2024-01-01 10:00:00')],
            'Bit_Wear': [1.5],    # Ù…Ø´Ú©Ù„: ÙØ±Ø³Ø§ÛŒØ´ Ø¨ÛŒØ´ Ø§Ø² 100%
            'Depth': [800],       # Ù…Ø´Ú©Ù„: Ú©Ø§Ù‡Ø´ Ø¹Ù…Ù‚
            'WOB': [30000],
            'RPM': [110],
            'ROP': [45]
        })
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬
        validator = DataQualityValidator()
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        results = validator.validate(test_data, enable_correction=True)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
        print("\nğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ:")
        print(f"âœ… ÙˆØ¶Ø¹ÛŒØª: {'Ù…Ø¹ØªØ¨Ø±' if results['is_valid'] else 'Ù†Ø§Ù…Ø¹ØªØ¨Ø±'}")
        print(f"ğŸ¯ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª: {results['quality_scores']['overall_score']}")
        print(f"ğŸ”§ ØªØ¹Ø¯Ø§Ø¯ Ø§ØµÙ„Ø§Ø­Ø§Øª: {len(results['correction_log'])}")
        
        if results['correction_log']:
            print("\nğŸ” Ø¬Ø²Ø¦ÛŒØ§Øª Ø§ØµÙ„Ø§Ø­Ø§Øª:")
            for log in results['correction_log']:
                print(f"   {log['field_name']}: {log['original_value']} â†’ {log['corrected_value']}")
                print(f"   Ø¯Ù„ÛŒÙ„: {log['problem_message']}")
        else:
            print("âš ï¸ Ù‡ÛŒÚ† Ø§ØµÙ„Ø§Ø­ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯!")
        
        return results

    def test_complete_system():
        """ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ"""
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
        sample_data = pd.DataFrame({
            'Timestamp': pd.date_range('2024-01-01', periods=10, freq='h'),  # 'h' Ú©ÙˆÚ†Ú©
            'WOB': [25000, 28000, 32000, 60000, 29000, 27000, 31000, 28000, 26000, 24000],
            'RPM': [120, 115, 125, np.nan, 118, 122, 119, 121, 117, 123],
            'ROP': [45, 48, 52, 150, 47, 46, 49, 51, 44, 50],
            'Standpipe_Pressure': [25000000, 25500000, 24800000, 25200000, 60000000, 25100000, 24900000, 25300000, 24700000, 25400000],
            'Depth': [1000, 1005, 1010, 1015, 1020, 1025, 1030, 1035, 1040, 1045],
            'Bit_Wear': [0.2, 0.25, 0.3, 1.5, 0.28, 0.32, 0.35, 0.38, 0.4, 0.42],
            'Maintenance_Flag': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
        })
        
        print("ğŸ§ª ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ...")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬
        validator = DataQualityValidator()
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        results = validator.validate(sample_data, enable_correction=True)
        
        print(f"\nâœ… Ù†ØªØ§ÛŒØ¬ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ:")
        print(f"ğŸ“Š ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ: {'Ù…Ø¹ØªØ¨Ø±' if results['is_valid'] else 'Ù†Ø§Ù…Ø¹ØªØ¨Ø±'}")
        print(f"ğŸ¯ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª: {results['quality_scores']['overall_score']}")
        print(f"âš ï¸  Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {results['summary']['critical_issues']}")
        print(f"ğŸ”§ Ø§ØµÙ„Ø§Ø­Ø§Øª Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡: {results['summary']['corrections_applied']}")
        print(f"â±ï¸  Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {results['validation_duration']:.2f} Ø«Ø§Ù†ÛŒÙ‡")
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø®Ø·Ø§Ù‡Ø§
        invalid_results = [r for r in results['validation_results'] if not r.is_valid]
        if invalid_results:
            print(f"\nğŸ” Ù†Ù…ÙˆÙ†Ù‡ Ø®Ø·Ø§Ù‡Ø§:")
            for result in invalid_results[:3]:
                print(f"   - {result.field_name}: {result.message} ({result.level.value})")
        
        return results

        # Ø§ÛŒÙ† Ú©Ø¯ Ø±Ùˆ Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„ data_quality_validator.py Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†

    def test_with_real_generator():
        """ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¨Ø§ Ø¬Ù†Ø±ÛŒØªÙˆØ± ÙˆØ§Ù‚Ø¹ÛŒ"""
        
        print("ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ØªØ³Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ø¬Ù†Ø±ÛŒØªÙˆØ± Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
        print("=" * 60)
        
        try:
            # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø¬Ù†Ø±ÛŒØªÙˆØ± - Ù…Ø·Ù…Ø¦Ù† Ø´Ùˆ Ø¯Ø± Ù…Ø³ÛŒØ± Ø¯Ø±Ø³Øª Ù‡Ø³Øª
            from advanced_drilling_generator import AdvancedSRSDataGenerator
            
            # Ù…Ø±Ø­Ù„Ù‡ Û±: ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù…ØµÙ†ÙˆØ¹ÛŒ
            print("ğŸ“Š Ù…Ø±Ø­Ù„Ù‡ Û±: ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø­ÙØ§Ø±ÛŒ...")
            generator = AdvancedSRSDataGenerator()
            
            # Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±ÛŒØ¹ØŒ Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ø±Ùˆ Ú©Ù… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            generator.duration_days = 1  # ÙÙ‚Ø· Û± Ø±ÙˆØ² Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±ÛŒØ¹
            generator.total_seconds = 24 * 3600  # 86400 Ø±Ú©ÙˆØ±Ø¯
            
            dataset = generator.generate_advanced_dataset()
            print(f"âœ… Ø¯Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {len(dataset):,} Ø±Ú©ÙˆØ±Ø¯")
            print(f"ğŸ“‹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(dataset.columns)}")
            
            # Ù…Ø±Ø­Ù„Ù‡ Û²: Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
            print("\nğŸ” Ù…Ø±Ø­Ù„Ù‡ Û²: Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©Ø§Ù…Ù„...")
            validator = DataQualityValidator()
            
            # ØªØ³Øª Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡ (Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±)
            sample_size = min(10000, len(dataset))
            test_sample = dataset.head(sample_size).copy()
            
            print(f"ğŸ“ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ {len(test_sample):,} Ø±Ú©ÙˆØ±Ø¯...")
            
            results = validator.validate(test_sample, enable_correction=True)
            
            # Ù…Ø±Ø­Ù„Ù‡ Û³: Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
            print("\nğŸ¯ Ù…Ø±Ø­Ù„Ù‡ Û³: Ù†ØªØ§ÛŒØ¬ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ")
            print("=" * 40)
            
            print(f"âœ… ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ: {'ğŸŸ¢ Ù…Ø¹ØªØ¨Ø±' if results['is_valid'] else 'ğŸ”´ Ù†Ø§Ù…Ø¹ØªØ¨Ø±'}")
            print(f"ğŸ“ˆ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª: {results['quality_scores']['overall_score']:.3f}")
            print(f"ğŸ·ï¸  ÙˆØ¶Ø¹ÛŒØª Ú©ÛŒÙÛŒØª: {results['quality_scores']['quality_status']}")
            print(f"â±ï¸  Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {results['validation_duration']:.2f} Ø«Ø§Ù†ÛŒÙ‡")
            
            print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ø®Ø·Ø§Ù‡Ø§:")
            print(f"   ğŸ”´ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {results['summary']['critical_issues']}")
            print(f"   ğŸŸ¡ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù‡Ø´Ø¯Ø§Ø±: {results['summary']['warning_issues']}") 
            print(f"   ğŸ”µ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ: {results['summary']['total_issues'] - results['summary']['critical_issues'] - results['summary']['warning_issues']}")
            print(f"   ğŸ”§ Ø§ØµÙ„Ø§Ø­Ø§Øª Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡: {results['summary']['corrections_applied']}")
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø·Ø§Ù‡Ø§
            if results['summary']['critical_issues'] > 0:
                print(f"\nğŸ” Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ (Ù†Ù…ÙˆÙ†Ù‡):")
                critical_errors = [r for r in results['validation_results'] 
                                if not r.is_valid and r.level == ValidationLevel.CRITICAL]
                for i, error in enumerate(critical_errors[:3]):
                    print(f"   {i+1}. {error.field_name}: {error.message}")
            
            return results
            
        except ImportError as e:
            print(f"âŒ Ø®Ø·Ø§: ÙØ§ÛŒÙ„ Ø¬Ù†Ø±ÛŒØªÙˆØ± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {e}")
            print("ğŸ’¡ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ ÙØ§ÛŒÙ„ advanced_drilling_generator.py Ø¯Ø± Ù…Ø³ÛŒØ± Ø¯Ø±Ø³Øª Ø§Ø³Øª")
            return None
# ================================
# UNIT TESTS
# ================================

import unittest

class TestConsistencyCorrection(unittest.TestCase):
    """Unit Test Ø¨Ø±Ø§ÛŒ Ù…ØªØ¯ Ø§ØµÙ„Ø§Ø­ Ù†Ø§Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ù…Ù†Ø·Ù‚ÛŒ"""
    
    def setUp(self):
        self.corrector = DataAutoCorrector()
    
    def test_bit_wear_overflow_correction(self):
        """ØªØ³Øª Ø§ØµÙ„Ø§Ø­ ÙØ±Ø³Ø§ÛŒØ´ Ù…ØªÙ‡ Ø¨ÛŒØ´ Ø§Ø² 100%"""
        test_df = pd.DataFrame({
            'Bit_Wear': [1.5],
            'Depth': [1000]
        })
        
        problem = ValidationResult(
            field_name='Bit_Wear',
            is_valid=False,
            validation_type="consistency_check",
            message="ÙØ±Ø³Ø§ÛŒØ´ Ù…ØªÙ‡ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨ÛŒØ´ Ø§Ø² 100% Ø¨Ø§Ø´Ø¯",
            level=ValidationLevel.CRITICAL,
            original_value=1.5
        )
        
        corrected_value, confidence = self.corrector._correct_consistency_issues(test_df, 'Bit_Wear', problem)
        
        self.assertEqual(corrected_value, 1.0)
        self.assertEqual(confidence, 0.9)

def run_simple_test():
    """ÛŒÚ© ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§"""
    print("ğŸ§ª Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø§Ø¯Ù‡...")
    
    test_data = pd.DataFrame({
        'Timestamp': pd.date_range('2024-01-01', periods=3, freq='h'),
        'Bit_Wear': [0.8, 0.9, 1.5],
        'Depth': [1000, 1005, 800],
        'WOB': [30000, 31000, 29000]
    })
    
    validator = DataQualityValidator()
    results = validator.validate(test_data, enable_correction=True)
    
    print(f"âœ… ÙˆØ¶Ø¹ÛŒØª: {'Ù…Ø¹ØªØ¨Ø±' if results['is_valid'] else 'Ù†Ø§Ù…Ø¹ØªØ¨Ø±'}")
    print(f"ğŸ”§ Ø§ØµÙ„Ø§Ø­Ø§Øª: {len(results['correction_log'])}")
    
    return True

if __name__ == "__main__":
    print("ğŸ¯ Ø³ÛŒØ³ØªÙ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­ÙØ§Ø±ÛŒ")
    
    success = run_simple_test()
    
    if success:
        print("ğŸ‰ ØªØ³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
    else:
        print("âŒ ØªØ³Øª Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯!")