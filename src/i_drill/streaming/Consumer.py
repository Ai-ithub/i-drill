# src/i_drill/streaming/consumer.py

import json
from confluent_kafka import Consumer, KafkaException
import logging

# ูุงุฑุฏ ฺฉุฑุฏู ูุงฺูู DVR ุงุฒ ูุณุฑ ุตุญุญ ุฏุฑ ุณุงุฎุชุงุฑ ุฌุฏุฏ
from i_drill.processing.dvr_controller import process_data

# --- ุฑุงูโุงูุฏุงุฒ ูุงฺฏูฺฏ ุจุฑุง ููุงุด ุจูุชุฑ ุฎุฑูุฌ ---
# ุงู ุจู ูุง ฺฉูฺฉ ูโฺฉูุฏ ุฎุฑูุฌโูุง consumer ู dvr_controller ุฑุง ฺฉุฌุง ุจุจูู
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# --- ุงุตูุงุญ ุชูุธูุงุช ฺฉุงูฺฉุง ุจุฑุง ฺฉุงุฑ ุจุง ุฏุงฺฉุฑ ---
KAFKA_BROKER = 'kafka:9092'  # ุงุณุชูุงุฏู ุงุฒ ูุงู ุณุฑูุณ ุฏุฑ docker-compose
TOPIC_NAME = 'oil_rig_sensor_data' # ุงุณุชูุงุฏู ุงุฒ ูุงู ุชุงูพฺฉ ุฌุฏุฏ ู ุฏุฑุณุช
CONSUMER_GROUP = 'dvr_processing_group'

def create_consumer():
    """ฺฉ Kafka consumer ุฌุฏุฏ ุงุฌุงุฏ ู ุขู ุฑุง ุจุฑุง ุฏุฑุงูุช ูพุงูโูุง ุขูุงุฏู ูโฺฉูุฏ."""
    consumer_conf = {
        'bootstrap.servers': KAFKA_BROKER,
        'group.id': CONSUMER_GROUP,
        'auto.offset.reset': 'earliest',
    }
    consumer = Consumer(consumer_conf)
    consumer.subscribe([TOPIC_NAME])
    return consumer

def consume_and_process():
    """
    ุจู ุตูุฑุช ูพูุณุชู ูพุงูโูุง ุฑุง ุงุฒ ฺฉุงูฺฉุง ุฏุฑุงูุช ฺฉุฑุฏูุ ุขูโูุง ุฑุง ุจุง ูุงฺูู DVR
    ูพุฑุฏุงุฒุด ูโฺฉูุฏ ู ูุชุฌู ุฑุง ููุงุด ูโุฏูุฏ.
    """
    consumer = create_consumer()
    logger.info(f"๐ฅ Listening to Kafka topic '{TOPIC_NAME}' for sensor data...")

    try:
        while True:
            msg = consumer.poll(timeout=1.0)
            if msg is None:
                continue
            if msg.error():
                raise KafkaException(msg.error())

            try:
                # ุฏุฑุงูุช ู ุฏฺฉูุฏ ฺฉุฑุฏู ูพุงู ุงุฒ ฺฉุงูฺฉุง
                data = json.loads(msg.value().decode('utf-8'))
                rig_id = data.get('rig_id', 'UNKNOWN_RIG')
                timestamp = data.get('timestamp', 'UNKNOWN_TIMESTAMP')
                logger.info(f"Received raw message from {rig_id} at {timestamp}")

                # >>>>>>>> ุจุฎุด ุงุตู ุชุบุฑุงุช: ุงุณุชูุงุฏู ุงุฒ DVR <<<<<<<<
                # ูพุงู ุฎุงู ุจู ูุงฺูู DVR ุจุฑุง ุงุนุชุจุงุฑุณูุฌ ู ุชุตุญุญ ุงุฑุณุงู ูโุดูุฏ
                reconciled_record = process_data(data)

                # ุจุฑุฑุณ ุฎุฑูุฌ ูุงฺูู DVR
                if reconciled_record:
                    # ุงฺฏุฑ ุฏุงุฏู ูุนุชุจุฑ ู ุชุตุญุญ ุดุฏู ุจูุฏุ ุขู ุฑุง ฺุงูพ ูโฺฉูู
                    logger.info("โ Record is VALID. Reconciled data:")
                    # ฺุงูพ ุฏุงุฏูโูุง ุชูุฒ ุจู ุตูุฑุช ุฎูุงูุง
                    print(json.dumps(reconciled_record, indent=2))
                else:
                    # ุงฺฏุฑ ุฏุงุฏู ูุงูุนุชุจุฑ ุจูุฏุ dvr_controller ุงุฒ ูุจู ุฏูู ุขู ุฑุง ูุงฺฏ ฺฉุฑุฏู ุงุณุช
                    # ูุง ุฏุฑ ุงูุฌุง ููุท ฺฉ ูพุงู ฺฉู ุซุจุช ูโฺฉูู
                    logger.warning("โ Record is INVALID and was discarded.")

            except json.JSONDecodeError:
                logger.error("Failed to decode JSON from message.")
            except Exception as e:
                logger.error(f"An unexpected error occurred: {e}")

    except KeyboardInterrupt:
        logger.info("๐ User stopped the consumer.")
    finally:
        consumer.close()
        logger.info("Kafka consumer closed.")

if __name__ == "__main__":
    consume_and_process()