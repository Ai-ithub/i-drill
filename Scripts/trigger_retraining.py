#!/usr/bin/env python3
"""
Script to trigger automated model retraining
"""
import argparse
import json
import logging
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional

sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import mlflow
    from src.backend.services.mlflow_service import mlflow_service
    from src.backend.services.model_validation_service import model_validation_service
except ImportError as e:
    print(f"âš ï¸ Required packages not available: {e}")
    sys.exit(1)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def check_retraining_conditions(
    model_name: str,
    conditions: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Check if retraining conditions are met.
    
    Conditions can include:
    - Performance degradation
    - Data drift
    - Time-based (periodic retraining)
    - New data availability
    """
    results = {
        "model_name": model_name,
        "should_retrain": False,
        "reasons": [],
        "timestamp": datetime.now().isoformat(),
    }
    
    if not mlflow_service or mlflow_service.client is None:
        logger.error("MLflow service not available")
        return results
    
    try:
        client = mlflow_service.client
        
        # Get latest production model
        model_versions = client.search_model_versions(f"name='{model_name}'")
        prod_version = None
        for version in model_versions:
            if version.current_stage == "Production":
                prod_version = version
                break
        
        if not prod_version:
            logger.warning(f"No Production version found for {model_name}")
            results["reasons"].append("No production model found")
            return results
        
        # Check time-based retraining
        if conditions.get("time_based", {}).get("enabled", False):
            interval_days = conditions["time_based"].get("interval_days", 30)
            last_training = datetime.fromtimestamp(prod_version.creation_timestamp / 1000)
            days_since_training = (datetime.now() - last_training).days
            
            if days_since_training >= interval_days:
                results["should_retrain"] = True
                results["reasons"].append(
                    f"Time-based: {days_since_training} days since last training "
                    f"(threshold: {interval_days} days)"
                )
        
        # Check performance degradation
        if conditions.get("performance_based", {}).get("enabled", False):
            threshold = conditions["performance_based"].get("degradation_threshold", 0.1)
            
            # Get current performance metrics
            run = client.get_run(prod_version.run_id)
            metrics = run.data.metrics
            
            # Compare with baseline
            baseline_metrics = conditions["performance_based"].get("baseline_metrics", {})
            for metric_name, baseline_value in baseline_metrics.items():
                if metric_name in metrics:
                    current_value = metrics[metric_name]
                    degradation = (baseline_value - current_value) / baseline_value
                    
                    if degradation > threshold:
                        results["should_retrain"] = True
                        results["reasons"].append(
                            f"Performance degradation: {metric_name} degraded by "
                            f"{degradation:.2%} (current: {current_value:.4f}, "
                            f"baseline: {baseline_value:.4f})"
                        )
        
        # Check data drift
        if conditions.get("drift_based", {}).get("enabled", False):
            drift_config = conditions["drift_based"]
            reference_data_path = drift_config.get("reference_data_path")
            production_data_path = drift_config.get("production_data_path")
            
            if reference_data_path and production_data_path:
                try:
                    import numpy as np
                    import pandas as pd
                    
                    ref_data = pd.read_csv(reference_data_path).values
                    prod_data = pd.read_csv(production_data_path).values
                    
                    drift_result = model_validation_service.detect_data_drift(
                        ref_data, prod_data,
                        threshold=drift_config.get("drift_threshold", 0.1)
                    )
                    
                    if drift_result.get("drift_detected", False):
                        results["should_retrain"] = True
                        results["reasons"].append("Data drift detected")
                
                except Exception as e:
                    logger.warning(f"Could not check data drift: {e}")
        
        # Check new data availability
        if conditions.get("data_availability", {}).get("enabled", False):
            data_path = conditions["data_availability"].get("data_path")
            min_samples = conditions["data_availability"].get("min_samples", 1000)
            
            if data_path and Path(data_path).exists():
                try:
                    import pandas as pd
                    data = pd.read_csv(data_path)
                    
                    if len(data) >= min_samples:
                        results["should_retrain"] = True
                        results["reasons"].append(
                            f"New data available: {len(data)} samples "
                            f"(threshold: {min_samples})"
                        )
                
                except Exception as e:
                    logger.warning(f"Could not check data availability: {e}")
    
    except Exception as e:
        logger.error(f"Error checking retraining conditions: {e}")
        results["error"] = str(e)
    
    return results


def trigger_training_workflow(
    model_name: str,
    model_type: str,
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Trigger model training workflow.
    
    In a real implementation, this would:
    - Create a GitHub Actions workflow dispatch
    - Submit a job to a training cluster
    - Queue a training job in a job scheduler
    """
    logger.info(f"ðŸš€ Triggering training workflow for {model_name}")
    
    # Placeholder - implement based on your infrastructure
    # Example: Use GitHub API to trigger workflow
    # Example: Use Kubernetes Job API
    # Example: Use Airflow API
    
    return {
        "triggered": True,
        "model_name": model_name,
        "model_type": model_type,
        "workflow_id": "placeholder",
        "timestamp": datetime.now().isoformat(),
    }


def main():
    parser = argparse.ArgumentParser(description="Trigger automated model retraining")
    parser.add_argument("--model_name", required=True, help="Model name to check")
    parser.add_argument("--model_type", help="Model type (auto-detected if not provided)")
    parser.add_argument("--config", required=True, help="Retraining conditions configuration JSON file")
    parser.add_argument("--auto-trigger", action="store_true", help="Automatically trigger training if conditions met")
    parser.add_argument("--check-only", action="store_true", help="Only check conditions, don't trigger")
    
    args = parser.parse_args()
    
    # Load configuration
    with open(args.config, "r") as f:
        conditions_config = json.load(f)
    
    # Check retraining conditions
    check_results = check_retraining_conditions(args.model_name, conditions_config)
    
    # Print results
    print(json.dumps(check_results, indent=2))
    
    # Trigger training if conditions met
    if check_results.get("should_retrain", False) and not args.check_only:
        if args.auto_trigger:
            # Determine model type
            model_type = args.model_type
            if not model_type:
                if "ppo" in args.model_name.lower():
                    model_type = "ppo"
                elif "sac" in args.model_name.lower():
                    model_type = "sac"
                elif "lstm" in args.model_name.lower():
                    model_type = "lstm"
                else:
                    logger.error("Could not determine model type. Please specify --model_type")
                    sys.exit(1)
            
            trigger_result = trigger_training_workflow(
                args.model_name,
                model_type,
                conditions_config
            )
            print(f"\nTraining triggered: {json.dumps(trigger_result, indent=2)}")
        else:
            logger.info("Retraining conditions met but --auto-trigger not specified")
            sys.exit(0)
    elif check_results.get("should_retrain", False):
        logger.info("Retraining conditions met (--check-only mode)")
        sys.exit(0)
    else:
        logger.info("Retraining conditions not met")
        sys.exit(0)


if __name__ == "__main__":
    main()

